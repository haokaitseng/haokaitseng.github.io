[{"categories":null,"contents":"Hi! My name is Hao Kai Tseng. I currently work with the Epidemic Intelligence Center at Taiwan Centers for Disease Control (CDC). My daily work involves outbreak monitoring across the globe, specifically on SARS-CoV-2, Polio, Cholera, and other food- and water-borne diseases.\nI was an intern at Gavi, the Vaccine Alliance on immunisation financing and sustainability. My work involved maintaining countries\u0026rsquo; Co-financing data across multiple databases, supporting key monitoring, analytics, and documentation.\nBefore joining Gavi, I studied Health Data Science Programme at the London School of Hygiene \u0026amp; Tropical Medicine (LSHTM). As a Student Liaison Officer of the Vaccine Centre at LSHTM, my career aspirations revolve around streamlining data pipelines in support of immunization initiatives - that\u0026rsquo;s where I am now!\nI acquired my BSc Public Health from the National Taiwan University in 2016. I have gained experience in the field of infectious diseases through my work in both a research institute and a governmental health sector Taiwan.\nI am interested in policy implementation and evaluation related to infectious diseases. The COVID-19 pandemic has heightened my focus on strengthening society\u0026rsquo;s capacity for pandemic prevention, preparedness, and response. Besides, I am actively engaged in assessing TB patient costs for WHO’s End Tuberculosis Strategy.\nAs a gay, promoting equity is my fav.\nIn an attempt to pull my skillsets to the cloud, this blog aims to offer practical applications and inspirations that give the push you need to explore the realms of public health and data science.\nI find fun in data, hope you do too!\n","date":"2025-03-30T20:18:54+03:00","permalink":"https://haokaitseng.github.io/about/","section":"","tags":null,"title":"About Me"},{"categories":["Power BI","Vaccine","Data Science"],"contents":" This post aims to advance your Power BI skills by:\nCreating a sequence of sustained measures in a new table, rather than having random measures co-existing with variables. Updating longitudinal data when new data is routinely received. [Note that the data used in this post are examples, inspired by my work with the Immunisation Financing \u0026amp; Sustainability (IF\u0026amp;S) team at Gavi. The views expressed here are my own and do not represent those of my former employer, Gavi.]\nPain points As a daily R user, it took me a few weeks to become handy with Power BI, a business intelligence solution that many global health organizations use to stimulate internal information. One of the pain points I encountered was that variables and created measures coexist under the same data but function quite differently. Let’s define the two:\nVariable: The original value as imported. It can be seen in both table and report views. It is not recommended to be used directly for report view, as it does not interact with slicers.\nMeasure: Created using DAX language, it is marked with a calculator icon. An indivisual measure is attached to the variable and only appears in report view, being invisible in table views.\nThere are significant differences between variables and measures. Why does Microsoft arrange them together alphabetically, creating a disorganized mess? Even worse, each created measure may undergo similar data processing steps, such as transforming and filtering. Why are we redundantly working on this?\nTo address these problems, in this post, I aim to make measures all reside under the same data, similar to creating a new data frame in R. Additionally, there must be a way to enhance measures’ reusability so that Power BI can be used as a data cleaning and management platform.\nThere must be a way to enhance measures\u0026rsquo; reusibility.\nSetting the scene Below are two hypothetical datasets, using the context from Gavi\u0026rsquo;s Co-Financing model:\nData A Each country runs several vaccine programmes. Each program has Co-Financing obligations in US dollars that must be fulfilled by the end of the year. Gavi and UNICEF jointly monitor payments against these obligations on a monthly basis. Data B Based on data A, the Co-Financing obligations and payments are aggregated at the country level, with programme-level details removed. KPIs, e.g. percent paid and status text, are summarized for each country. A monthly snapshot is taken and appended to previous records. In short, every month, when Gavi IF\u0026amp;S team get the new data (data A), it is processed into data B.\nA new table that has everything I promise to provide a table with all the measures that you need. Let\u0026rsquo;s break it into 4 steps:\nStep 1 In Power BI’s Table View, navigate to the Calculations tab in the top panel and click the New Table icon. Then, write a DAX expression like the one below to create a new table called New Table A.\nHere, we summarize two variables, total obligation and amount paid, by country name and iso code using data from Data A. Note that this summarized information is temporarily stored as table_1 using the VAR function.\nNew table A = //Parameters to check/adjust every time:\rVAR month_of_the_report = \u0026#34;01/02/2025\u0026#34; // DD/MM/YYYY VAR table_1 = SUMMARIZE(\r//Main data:\r\u0026#39;Data A\u0026#39;\r,\r//Group By:\r\u0026#39;Data A\u0026#39;[country],\r\u0026#39;Data A\u0026#39;[iso],\r//Variables:\r\u0026#34;total obligation\u0026#34;, SUM(\u0026#39;Data A\u0026#39;[total obligation]),\r\u0026#34;amount paid\u0026#34;, SUM(\u0026#39;Data A\u0026#39;[amount paid])\r) Step 2 Following the DAX expression, we enter the DAX code below. This chunk directly utilizes the two measures created in table_1 in Step 1—[amount paid] and [total obligation]—rather than raw variables from Data A.\nIn other words, we reuse these measures to calculate the [percent paid] measure and temporarily store it in another VAR called table_2.\nVAR table_2 = ADDCOLUMNS(\rtable_1,\r\u0026#34;percent paid\u0026#34;, ROUND( DIVIDE([amount paid], [total obligation]), 2) // ranging 0-1\r) Step 3 Within the same DAX expression, we reuse the measure created in the previous step — [percent paid].\nWe then apply the SWITCH function to categorize [percent paid] into different levels, creating a new categorical measure called [status]. This [status] measure is temporarily stored in the VAR table_3.\nVAR table_3 = ADDCOLUMNS(\rtable_2,\r\u0026#34;status\u0026#34;, SWITCH( TRUE,\r[percent paid] \u0026gt;= 1, \u0026#34;Fully paid\u0026#34;,\r[percent paid] \u0026gt; 0.75, \u0026#34;Partial paid for more than 75%\u0026#34;,\r[percent paid] \u0026gt; 0.5, \u0026#34;Partial paid for more than 50%\u0026#34;,\r[percent paid] \u0026gt; 0.25, \u0026#34;Partial paid for more than 25%\u0026#34;,\r[percent paid] \u0026gt; 0, \u0026#34;Partial paid less than 25%\u0026#34;,\r[percent paid] = 0, \u0026#34;Not paid yet\u0026#34;,\r\u0026#34;other\u0026#34; )\r) Step 4 In the final step, we use the RETURN function to output the information temporarily stored in VAR.\nSince we need to generate a table, we also use the SELECTCOLUMNS function to extract the relevant measures. Right after the SELECTCOLUMNS, You can adjust the text inside the brackets \u0026quot;\u0026quot; to rename your measures as needed.\nRETURN\rSELECTCOLUMNS(\rtable_3,\r\u0026#34;iso\u0026#34;, [iso],\r\u0026#34;country\u0026#34;, [country],\r\u0026#34;status\u0026#34;, [status],\r\u0026#34;total obligation\u0026#34;, [total obligation],\r\u0026#34;amount paid\u0026#34;, [amount paid],\r\u0026#34;percent paid\u0026#34;, [percent paid],\r\u0026#34;month of the report\u0026#34;, month_of_the_report\r) Important Distinction: VAR vs. Measures\n[status] is a measure (denoted by brackets []), meaning it can be used in the Report View. month_of_the_report is a VAR, which only exists temporarily within this DAX expression and is NOT available or accessible in Report View. Below is the resulting table, New Table A, created through these four steps. Updating on a monthly basis Sanpshots To update table B, we now need to take a snapshot of new table A and append it to table B. This can be easily done using the UNION function.\nNew table B = UNION(\u0026#39;Data B\u0026#39;, \u0026#39;New table A\u0026#39;) Once new table B is updated, we should export the data from Power BI for historical record. In the Table view, right click the new table B, and paste the data into a blank excel file.\nUpdating from February to March Suppose your Power BI is using February data, and you just received the March data A. Here\u0026rsquo;s how to update the data from February to March. The graph below illustrates the steps:\nNavigate to the File button in the top-left panel and click Options and settings. Select Data source settings. Select \u0026ldquo;data a February 2025.xlsx,\u0026rdquo; press the Change source button, and replace it with \u0026ldquo;data a march 2025.xlsx.\u0026rdquo; Select \u0026ldquo;data b January 2025.xlsx,\u0026rdquo; press the Change source button, and replace it with \u0026ldquo;data b February 2025.xlsx.\u0026rdquo; Go to the new table A\u0026rsquo;s DAX expression. Once the date for month_of_the_report is updated to \u0026ldquo;01/03/2025,\u0026rdquo; the snapshot for March will be automatically completed. In parallel, new table B will also be updated, with March data appended. Export the new table B from Power BI. Note that these steps use Change source rather than refresh function - as refresh only updates the original, fixed data source.\nWhat do you mean by \u0026ldquo;self-sustained\u0026rdquo;? In summary, the way the data is updated is self-sustained because:\nIn the new table A, measures are set to sustain its usage into next VAR. This is beneficial for a set of order-sensitive measures that rely on the same filtering, thereby saving space by eliminating redundant filtering processes.\nAll data processing steps for updating the new table B are carried out within Power BI, while also preserving a track record. In other words, this self-sustained process can continue autonomously once initiated, as long as new data is fed into it.\nWith these self-sustained concepts in the Table View, you can create fantastic visuals in Report view while ensuring concise, clear, organized, and less error-prone data management for routine tasks.\nThis self-sustained concepts align with my core values in data science: data efficiency, integrity, and reproducibility.\nAfter working on this, I do feel developing a Power BI dashboard doesn\u0026rsquo;t mean low-code at all\u0026hellip; Any thoughts?\nThis DAX code was collaborated with Copilot. The wording in this post is refined by ChatGPT. Views expressed here are my own and don\u0026rsquo;t represent the views of my former employer, Gavi. ","date":"2025-03-13T22:11:12+08:00","permalink":"https://haokaitseng.github.io/post/20250313_powerbi_sustained_data_updating/","section":"post","tags":null,"title":"Self-sustained data \u0026 measures in Power BI"},{"categories":["Portfolio Management","Vaccine","Visualisation","Data Science","R"],"contents":" This post shares my work at the lovely Immunisation Financing \u0026amp; Sustainability (IF\u0026amp;S) team at Gavi, the Vaccine Alliance. Please note that views expressed here are my own and don\u0026rsquo;t represent the views of my employer, Gavi. It covers:\nBrief intro for Gavi\u0026rsquo;s Co-financing model 5 visual tips for portfolio management (some with R code), e.g. risk-assessment table and faceted pie An optimized data flow for a complex data ecosystem What is Co-financing Under Gavi\u0026rsquo;s Co-financing policy, countries share the costs of the vaccine programmes by directly co-procuring a portion of the vaccines and injection devices from a supplier or procurement agency to fulfill their co-financing requirements. This policy, along with the Eligibility and Transition policies, aims to bring countries on a trajectory towards financial sustainability. The Co-financing level is associated a country\u0026rsquo;s capacity to pay, i.e. GNI per capita.\nEach year, the primary objective of the Immunisation Financing \u0026amp; Sustainability (IF\u0026amp;S) team is to ensure that Gavi-implementing countries\u0026rsquo; co-financing commitments are transformed into actual funding allocations.\nExternally, IF\u0026amp;S partners with 54 countries for immunisation budget planning, advocacy, and technical assistance, and resolve issues related to payment and transition. Internally, IF\u0026amp;S monitors monthly payment progress and identify at-risk countries for intensified engagement. We believe every Co-financing dollar counts, empowering governments to take ownership of their vaccination programmes.\nManaging a portfolio of US$ 200+ million Take 2023\u0026rsquo;s Co-financing. Gavi has a large portfolio consisting of many kinds of vaccine products, e.g. PCV, HPV, and Penta. The portfolio\u0026rsquo;s Co-financing payments amounted US$ 215 million in 2023. It brought to US$ 1.7 billion countries total contribution since the introduction of the Co-financing policy in 2008.\nIt has become challenging to oversee such large scale of funding streams with an expanding portfolio. Close monitoring of countries\u0026rsquo; co-financing status (Outstanding, Partially fulfilled, Fully fulfilled) is key to effective management.\nThanks to the power of analytics tools and routine practices, here are five methods that we used to get the jobs done: time series, risk table, facet pie, Alluvial/Sankey, and treemap.\n1. Time series monitoring We compare this year\u0026rsquo;s Co-financing progress with previous five years as KPIs. The KPIs can be absolute dollar paid or number of countries (partially) fulfilled.\nTiming is a critical factor for Co-financing. For two reasons:\nFor countries using calendar years, 31 December is regarded as their deadlines. If countries don\u0026rsquo;t take Co-financing seriously, they could be chronically late payer. The co-financed vaccine doses will be shipped and delivered once the co-financing payment is made. In other words, no pay, no gain. Delayed payments may result in vaccine stockouts if the stockpile is lower than the consumption threshold. No pay, no gain!\n2. Risk assessment table Country Co-financing obligations Co-financing paid Share of paid Co-financing remaining Co-financing status Risk Actions Next steps Country A $200 $196 98% NA Fully fulfilled No risk The country has met on doses according to UNICEF\u0026rsquo;s cost estimate. Weekly coordination meeting with the country. Country B $100 $80 80% $20 Partially fulfilled Medium risk Reminding letter has been sent. High level meeting This table dives into the individual countries, combining qunatative and qualitative data for Co-financing. Since there are serious implications of defualting, i.e. not fully fulfilled, we use this table with another detailed monitoring table (not shown here) to assess which countries/antigens remaining unpaid.\nCoutnry A: Why the country only paid 98% but considered fully fulfilled? This is because the country has procured all the required co-financed vaccine doses. There is often a difference between the Co-financing obligation dollars (estimated by Gavi) and the cost estimate (provided by UNICEF or PAHO as the procurement channel). Changing freight costs and insurance are the contributing factors to the cost gap.\nCountry B: For countries that still have outstanding antigens to be paid, Gavi sends an official letter to remind them of the remaining amount. Additionally, a higher-level meeting may be arranged with the Ministry of Health or Ministry of Finance to understand the fiscal space and nudge the transaction. Opening windows of communication is crucial, as common reasons for outstanding payments include macroeconomic headwinds, government political transitions, and fragile or humanitarian contexts.\nQualitative data aids in assessing the probability of successful Co-financing payment for the portfolio.\n3. Faceted pie charts for portfolio composition To fully understand the portfolio picture, it is helpful to visualize the Co-financing by country and by antigens. This can be achieved using the facet_grid() argument in ggplot2, allowing people to quickly identify which vaccines are available in each country.\nAdditionally, generating pie charts with multiple categorical components can aid in understanding the share of each component. For example, in the context of Co-financing, two colors can be used to represent the paid amount and the remaining amount.\nggplot(data,\raes(x = \u0026#34;\u0026#34;, y = cost_normalized, fill = category)) +\rgeom_col() +\rcoord_polar(theta = \u0026#34;y\u0026#34;)+\rfacet_grid(country ~ vaccine,\rswitch = \u0026#34;y\u0026#34;)+\rtheme_void() 4. Alluvial / Sankey diagram for multi-level streams Alluvial and Sankey diagrams are a type of flow diagram showing changes in group composition between category fields. The thickness of each stream indicates its proportional value. These diagram can help us digest:\nCo-financing amounts by antigen and Co-financing amounts by funding sources. The funding sources can be directly domestic, UNICEF\u0026rsquo;s special mechanism, external (e.g. World Bank) A country\u0026rsquo;s linking chain of GDP, GGE(General Genvernment Expenditure)-GDP, GGHE(General Government Health Expenditure)-GGE, immunisation-GGHE. Graph credit: ggsankeyfier package\n5. Treemap for landscaping A treemap is an excellent way to demonstrate relative quantities and categories simultaneously. By building upon your self-defined groups, e.g. from 1 to 4 in the graph below, it can be strategically used to select a subset of data, determining the scope of the portfolio optimasation or strategy that you would like to work on.\nlibrary(treemapify)\rggplot(data, aes(area = cost,fill = vaccine,label= vaccine,subgroup = factor(flag))) +\rgeom_treemap()+\rgeom_treemap_text(colour = \u0026#34;white\u0026#34;,place = \u0026#34;centre\u0026#34;,size = 15,grow = TRUE)+\rgeom_treemap_text(aes(label = cost),colour = \u0026#34;white\u0026#34;,place = \u0026#34;bottomright\u0026#34;,size = 13,alpha = 0.8) +\rgeom_treemap_subgroup_text(place = \u0026#34;centre\u0026#34;,grow = TRUE,alpha = 0.25,colour = \u0026#34;black\u0026#34;,fontface = \u0026#34;italic\u0026#34;) Optimizing data flow for a complex data ecosystem The majority of the tasks at Gavi are built and transfered on Microsoft Office applications, such as Excel and PowerPoint. To make your data life easier and less manual, it is essential to bridge across different data processing steps, including:\nData sources: Salesforce, SAP, emailed Excel files, websites (e.g. WHO API) Saved raw data: often in Excel format or CSV format Programming tool for data processing: R (studio) for cleaning and analytics Intermediate: cleaned file in Excel format or RData Dashboard: in interactive Power BI Presentation: using static or live-embedded graphics from Power BI for PowerPoint deck Publishing: in paramaterized Quarto report or Power BI pagenated report Deliverabels: graphics in PNG file produced from R, or inside Excel I\u0026rsquo;ve applied my Extract, Transfrom, Load (ETL) skills in a previous post at LSHTM. This time at Gavi, the challenge is more complex. To optimize the data flow, concepts for reproducibility and interoperability should be the focus, along with simplicity. Using R as a centralized programming deployment can improve efficiency, ensure consistency, minimize maintaince efforts while dealing with frequent portfolio changes. Notably, building the connection between Power BI and PowerPoint can dramatically save time, where the data flow can be integrated in Power BI itself.\nTo make your data life easier and less manual, it is essential to bridge across different data processing steps.\nAdditionally, this data flow can be used for continuous improvement of operational processes, as data can be collected from SAP sources, consumed into R, and presented in Power BI/Powerpoint visuals.\nI thank my supervisor Anthony Nguyen for building up the foundation for this post and for his pioneering insights. Yet, I am still exploring how to integrate AI as a text summarizing tool for qualitative data in this pipeline. Views expressed here are my own and don\u0026rsquo;t represent the views of my employer, Gavi. ","date":"2024-12-31T22:11:12+08:00","permalink":"https://haokaitseng.github.io/post/20241220-cofinancing-portfolio-management/","section":"post","tags":null,"title":"Co-financing for vaccine portfolio: visual tips and data flow"},{"categories":["Data Science","Vaccine","Polio","Bayesian","R"],"contents":" This post shares my poster presentation at the Vaccine Hesitancy Course held by the Croucher Foundation in August, 2024, Hong Kong.\nDownload the poster here, which is the milestone of my thesis project at LSHTM. Introduction Global incidence of polio has decreased by 99% since 1988. Non-polio Acute Flaccid Paralysis (NPAFP) is the key indicator for the sensitivity of polio surveillance. While countries with circulating poliovirus consistently report NPAFP rates exceeding the gold-standard indicator*, it is not well-understood what rate should be expected. This project aims to identify predictive features and model expected NPAFP rates in support of polio eradication. Method Global Polio Eradication Initiative’s (GPEI) NPAFP data in Afghanistan, Pakistan, and Nigeria are dependent variable. Predictors are derived from open data, including vaccine confidence index (% agree that vaccines are effective), SIAs (supplementary immunisation activities), and vaccine coverage. Integrated nested Laplace approximation (INLA) is used for approximate Bayesian inference. Building on a negative binomial framework, population serves as the model’s offset. Regarding spatial and temporal dependency, Besag-York-Mollié (BYM) and autoregressive models (1) are used, respectively. A penalized complexity (PC) prior on the precision is specified. Monthly aggregated scaled data at admin 2 level from 2018 to 2020 are fitted to predict NPAFP cases between 2021 and 2023. Result In the past 3 years, the observed average annual NPAFP rates were 23 in Afghanistan, 18 in Pakistan, and 9 in Nigeria per 100,000. The predicted rates are 17, 15, and 8, respectively. Polio vaccine coverage among 1-year-olds is the strongest positive predictor for NPAFP. SIA and % agree that vaccines are effective have a negative posterior mean for NPAFP. In 2021, the mean differences of predicted and observed annual NPAFP rates among districts are -7.4, -0.6, 0.7 per 100,000 for Afghanistan, Pakistan, and Nigeria, respectively. Discussion In comparison of predicted and reported NPAFP rates, yellow areas on the maps surrounded by polio confirmed cases (red dots) may indicate at-risk regions with lower sensitivity of the AFP surveillance systems. Predictors may provide insights into the NPAFP notification process, covering aetiologies of AFP-causing illness, accessibility \u0026amp; utilization of healthcare services, and reporting practices. The polio eradication and certification efforts need to reassess the decade-old “gold-standard indicator”. The mechanisms linking vaccine confidence and coverage to AFP reporting requires further investigation. Conclusion With both observed and predicted NPAFP rates greatly surpassing the indicator, areas with lower sensitivity of polio surveillance require attention.\nI thank the scholarship sponsored by the Croucher Foundation for attending the course, with thanks to Dr Emily Nightingale for her supervision and Prof Edward Parker for writing the recommendation letter for me.\n","date":"2024-07-18T22:11:12+08:00","permalink":"https://haokaitseng.github.io/post/20240719_polio_poster_vaccine_hesitancy/","section":"post","tags":null,"title":"Mapping the Relationships: Modelling Immunisation Efforts on the Non-Polio Acute Flaccid Paralysis Rate"},{"categories":["Data Science","Vaccine","Essay"],"contents":" This post shares my two publications at LSHTM, showcasing my two student roles and achievements:\nStudent and Digital Content Ambassador Student Liason Officer at the Vaccine Centre Songs That Accompany Our Study In this blog post, I share the diverse music landscape shaped by my MSc Health Data Science classmates from across the globe. This post is also being highlighted on the World Music Day, 21th June 2024, by the LHSTM channel of Instagram and Facebook.\nTo listen to our music, please navigate to our public Spotify Playlist. This features 16 songs in 5 languages from various genres, each accompanied by the story behind it.\nMeet the Student Liason Officers Serving as the two Student Liason Officers of the Vaccine Centre 2023/24, Ashna and I co-wrote the member profilling for May to the Vaccine Centre. In this post, we talked about our contributions to the work of the centre, our research interests, as well as some amazing achievements and wonderful developments in our personal lives.\nI thank my fellow Student Liason Officer Ashna for co-writing and my Health Data Sciecne classmates for reccommending their favorite songs, including Mareca, Joe, Jasjot, Patricia, Guzzi, Louis, Jenny, Nikki, Lamprini, Angel, Kit Ying, Dhihram, Lana, Soe, and Giovanni.\n","date":"2024-06-13T22:11:12+08:00","permalink":"https://haokaitseng.github.io/post/20240614_hdsplaylist_vacfocus/","section":"post","tags":null,"title":"My MSc Journey at LSHTM"},{"categories":["Data Science","Visualisation","Vaccine","COVID-19","Flu","RSV"],"contents":" This post shares a publication on Think Global Health, showcasing collaborative efforts with my teammamtes: Wan-Jen Lee, Giovanni Jacob, Dhihram Tenrisau, and Caitlynne McGaff.\nRead our article by navitagting to Think Global Health. Data Visualisations Epicurve in England This graph has been recreated by Editor Allison Krugman.\nSelected Heatmaps Heatmaps showing the log hospitalization rates for influenza and RSV throughout each season. Darker shades means higher hospitalization rate. This graph has been recreated by Editor Allison Krugman.\nPathgraph Overview Hospitalization Peak in Each Season Dot represents the highest hospitalization rate within the season, starting from week 19 for northern hemisphere and week 45 for southern hemisphere. A line indicates the peak trends for each country from the earliest to the latest season. This graph was created by me, although it wasn\u0026rsquo;t ultimately included in the publication.\nSummary The COVID-19 pandemic disrupted the transmission of respiratory pathogens, including influenza (flu) and respiratory syncytial virus (RSV). Influenza hospitalisation peaks have been occurring 4.3 weeks earlier in five temperate countries (England, France, Türkiye, US, and Australia) since the 2019-2020 season. RSV hospitalisation peaks were more erratic, arriving 1.3 weeks earlier on average in the five temperate countries after the pandemic. The WHO may need to consider convening consultations earlier for influenza vaccine composition due to the observed shifts. Several factors have potentially influenced the observed shifts, including contact patterns, public health social measures, and immunity (decline in vaccine uptake, changes in vaccination attitudes, and vaccine fatigue). Year-round monitoring for multiple pathogens with overlapping symptoms profiles and reporting to global surveillance databases are essential for timely planning and delivery of health protection programs. Data Source This article is adapted from the assignment of the Data Challenge module at LSHTM. We thank Associate Professor Rosalind Eggo for her expert guidance and thank Associate Professor Thomas Cowling and Assistant Professor Simon Procter for making this possible. Also huge thanks to editors Nsikan Akpan and Allison Krugman.\n","date":"2024-05-13T22:11:12+08:00","permalink":"https://haokaitseng.github.io/post/20240514_sanofi_think_global_health/","section":"post","tags":null,"title":"Where COVID Has Shifted Flu and RSV Seasons"},{"categories":["Data Science","Polio","Vaccine","Machine Learning"],"contents":" This post presents my research proposal for my MSc thesis: \u0026ldquo;Predicting the rate of Acute Flaccid Paralysis in different settings to support polio surveillance and elimination\u0026rdquo;.\nYou will learn why non-polio Acute Flaccid Paralysis (NPAFP) is important in support of polio eradication. The current challenge involves interpreting the AFP indicator that exceed the targeted levels. Potential data sources and modelling methods are raised to fill this knowledge gap. Aim and Objectives Research Aim To investigate and model the mechanisms underlying the notification of the key polio surveillance indicator - NPAFP rates in endemic and outbreak settings.\nResearch Objectives Review and consolidate existing evidence about the possible aetiologies of AFP in population aged less than 15. Identify the most informative predictive features for NPAFP rates in WPV-1 endemic countries and a selection of cVDPV outbreak countries. Building on the potential causal mechanism of AFP, modelling the expected NPAFP rates at both national and subnational levels. Background Polio Today Since the global efforts initiated in 1988, wild poliovirus has been very close to global eradication. Endemic transmissions of wild poliovirus type 1 (WPV-1) is restricted to certain regions in Afghanistan and Pakistan. There are also episodes of circulating vaccine-derived poliovirus (cVDPV), mostly notified in World Health Organisation African region (AFRO).\nThe WHO Global Polio Eradication Initiative (GPEI) sets the gold standard for detecting cases of poliomyelitis. The key indicator, the sensitivity of surveillance, is defined at least one case of non-polio AFP should be detected annually per 100,000 population aged less than 15 years. This is the minimum level for certificating a standard surveillance system, which indicate the probability that polio infection will be detected given that infection is present in the population. To ensure higher sensitivity in endemic regions and during polio outbreaks, the NPAFP rate should be two per 100,000 and three per 100,000, respectively (GPEI, 2021).\nAFP Notification AFP is characterized by repaid onset of gait disturbance, weakness, or trouble coordination in one or several extremities. The symptoms can progress to maximum severity within 10 days, frequently encompassing weakness of the muscles of respiration and swallowing. Every AFP case, as a result, poses a clinical emergency. It requires immediate neurologic examination to narrow the differential diagnosis, often involving electrophysiologic studies (Marx et al., 2000).\nPoliomyelitis is not the only cause of AFP. There many other causes, including Guillain-Barre syndrome, non-polio enterovirus, other neurotropic viruses, and acute traumatic sciatic neuritis. These potential causes of AFP are often associated with anatomic-morphologic changes or specific pathophysiologic mechanisms. Confirmation of either polio or NPAFP can only be achieved through collecting and testing of stool specimens from AFP cases within the Global Polio Laboratory Network (Tangermann et al., 2017). In other words, appropriate testing AFP, regardless of its presumed etiology, is an integral component of polio surveillance (Marx et al., 2000).\nFrom a macro perspective, the reporting of NPAFP may be associated with multiple factors within a sequence of events. These events include: (1) the necessity for a susceptible individual to be effectively exposed to the pathogens or causes mentioned above, (2) onset of symptoms, (3) seeking healthcare (accompanied by parents), (4) notification as AFP, (5) collection of stool specimens, (6) sample transportation, and (7) laboratory examination (Tangermann et al., 2017).\nAcross these sequence of events, factors such as dwelling environment (related to climate, sanitation, and access to healthcare services), susceptibility (related to immunity and underlying prevalence), strengths of the surveillance system (related to technical, laboratory ,human, and financial resources) may play crucial roles. For instance, Pakistan and India’s NPAFP rates have been observed as having some association with polio vaccine uptakes and polio cases, respectively (Dhiman et al., 2018; Molodecky et al., 2017). Besides, factors associated with an increase risk of poliovirus outbreaks have also been identified (O’Reilly et al., 2017). The interactions between polio campaigns and health systems were also illustrated through a causal loop diagram (Neel et al., 2021). Nevertheless, there remains a limited understanding and approach to quantify the associations between AFP notifications and various factors throughout the continuum of surveillance events, let alone establishing causal inference.\nChallenge The current gold standard of NPAFP rate to define a well-operating poliovirus surveillance system might be challenged. First, from the perspective of certification, zero-reporting doesn\u0026rsquo;t necessarily imply the absence of the underlying (latent) numbers of polio cases in a population. For example, an incorrect declaration of polio elimination almost occurred in Nigeria in 2016, following a track record of 730 days polio-free (Adamu et al., 2019; Bell, 2019).\nSecond, in response to outbreaks, a cross-country study indicates that maintaining a high rate of NPAFP is essential to timely detection of cVDPVs outbreaks (Auzenbergs et al., 2021). It is critical for community health workers in outbreak regions to know how effectively the AFP surveillance systems monitor and identify cases, coupled with environmental surveillance.\nThird, concerning international movements, no country can be exempt from the risk of imported polio cases. A WPV-1 confirmed case was reported in Malawi in 2017, which was genetically linked to WPV-1 transmission in Pakistan (2022). This underscores the ongoing risk of international spread until the objective of global polio eradication is attained. Countries that fail to report NPAFP rates after certification may require assessment.\nStatistics According to the WHO’s open data (Figure 1), it is observed that in 2000, the NPAFP rate falling within the range of 1 to 2 per 100,000 accounted for the largest share (43%) among the reporting countries. Over the years, there has been an increase in the proportion exceeding 2 per 100,000, with 77% of countries’ NPAFP rates surpassing this standard in 2023.\nDiving into the data distribution, the median NPAFP rate of the annually median from the reporting countries is 1.7 per 100,000. However, this century ‘s maximum NPAFP rates by year range from 3.8 to 373.9 per 100,000. This wide range is accompanied by an increasing standard deviation in recent years. This right-skewed distribution of data poses a challenge to properly interpretate changes in NPAFP rates, as it reflects different strength of surveillance in different contexts. Notably, the aftermath of the COVID-19 pandemic could potentially shapes the healthcare landscape, as well as the polio surveillance systems (Niaz et al., 2023).\nNPAFP rate (2000-2023) Min Q1 Median Mean Q3 Max Annual Median 1.3 1.5 1.7 2.5 1.9 19.3 Annual Mean 1.4 1.9 2.3 3.8 2.8 37.2 Annual Maximum 3.8 9.2 15 30.4 22 373.9 Annual SD 0.8 1.4 2.2 4.5 3.4 53.6 Mission Building on the partnership with the GPEI (on behalf of WHO), this project uses GPEI’s database, originally owned by the United Nations Children\u0026rsquo;s Fund (UNICEF). The AFP data (including polio and non-polio cases) cover a comprehensive time (\u0026gt; 20 years) and space, including WHO AFRO, Eastern Mediterranean Region (EMRO), and Western Pacific Region (WPRO). For the purpose of polio eradication, this project’s data landscape focuses on the WPV-1 endemic countries, Afghanistan and Pakistan, coupled with cVDPV outbreak countries, such as Nigeria. Publicly available data will also be collected and analysed as potential predictors at national and subnational levels.\nThe added value of this project is to offer insight into the reconsideration of GPEI’s key surveillance indicator - NPAFP rate. This project can also contribute to the component of the Free From Infection (FFI) framework of the broad SPEC project. By understanding the sensitivity of the different components that make up the detection process, this project aims to answer what rate of NPAFP should be expected in the under-15 group, either for retrospective data analysis or for future projections. If study countries consistently reports zero polio cases for a certain period, with NPAFP reporting rate exceeding the expected NPAFP rate, this can serve as evidence of the successful interruption of poliovirus transmission.\nADDED VALUE: By assessing the target rates of reported NPAFP among the under 15 population in endemic regions, this project\u0026rsquo;s value lies in defining a well-operating surveillance system, and identifying regions at risk of under detection.\nFurthermore, through the comparison between reported and expected NPAFP rates, it can also identify the weakness of surveillance systems and informs resource mobilisation during different phases, including outbreaks, eradication, and post-certification. Therefore, within the framework of the Polio Eradication Strategy 2022-2026 facilitated by GPEI, this project has scientific importance in providing essential support for surveillance activities and certification outlined in the strategy\u0026rsquo;s timeline.\nMethods Outcome Measures The NPAFP rate per 100,000 in children aged less than 15, which is GPEI’s number of NPAFP cases divided by the annual under-15 population size according to national statistics.\nPredictors The absolute number of AFP cases is likely to vary substantially between populations due to a number of factors. Predictors to be assessed including:\nGeographical: (1) urban, rural, or conflict zone, (2) districts bordering, (3) temperature, (4) precipitation, (5) risk of diarrhea. Demographic: (1) population size: fewer than 50,000 children under 15 years of age may not detect AFP every year (WHO, 2022), (2) population density, (3) migration and movement (displacement), (4) socio-economic (poverty and barriers to access), (5) total births, (6) gender proportions. Surveillance and healthcare system: (1) healthcare services quality, accessibility, and affordability (out-of-pocket payment), (2) Electronic surveillance technology, (3) sensitivity of laboratory testing methods, (4) environmental surveillance, (5) number of SIA campaigns, (6) human resources of community health workers, (7) health financing, (8) performance assessment or reward for reporting, (9) unit of reporting (number of offices/hospitals), (10) pre-and-post COVID-19 pandemic. Biological: (1) non-polio enterovirus rate, (2) Guillain-Barré Syndrome prevalence, (3) polio confirmed cases and type. Immunisation: (1) OPV and IPV vaccination coverage, (2) OPV and IPV vaccines administered, (3) switch from tOPV to bOPV. Models The time interval within the scope of the method may be monthly, 6-month, and annually, whilst geographical scale may be applied to district, country, and continent level. A sequential process from risk exposure, probability of present AFP disease, seeking healthcare, notification, stool collection, sample transportation, to laboratory results will be considered throughout the analysis. The models to be implemented are as follows:\nMachine learning methods, which are robust to high dimensionality and potential dependencies between predictors (dependent on what kind of indicators can be obtained across all geographies). This may include penalised regression (Lasso and Ridge), tree-based methods (random forest), or super learner. These methods focus on learning complex and non-linear relationships. They can assist in subset selection and variables selection, given the wide range of variables in this project. They can adapt to intricate patterns but may be prone to overfitting, especially with limited data; hence it is crucial to find a bias-variance trade-off.\nRegression based approaches accounting for spatial and temporal dependence. This may include generalized additive models (GAMs), splines, autoregressive models, and Poisson mixed-effect model. These methods focus on parameter estimation based on assumed relationships, also allowing non-parametric fits with relaxed assumptions. Past values of the variable of interest may be considered. Coefficients in these models may directly represent the impact of each variable on the dependent variable.\nMethods to evaluate out-of-sample predictive ability of models. It serves as an validation purpose. This MSc summer project is under the SPEC (Surveillance Modelling to Support Polio Elimination and Certification) project work package 1: Surveillance Modelling, led by Dr. Kathleen O\u0026rsquo;Reilly and supervised by Dr. Emily Nightingale at LSHTM.\n","date":"2024-05-03T22:11:12+08:00","permalink":"https://haokaitseng.github.io/post/20240503_polio_proposal/","section":"post","tags":null,"title":"The Beginnig of the End: What Is the Expected Notification Rate of the Polio Surveillance Indicator?"},{"categories":["Vaccine"],"contents":"For poeple doing vaccination works, their world wareness day is not a single day - it\u0026rsquo;s a week, spanning from 24th to 30th April each year.\nThis year, WHO celebrate 50 years of the \u0026ldquo;Expanded Programme on Immunization (EPI)\u0026rdquo;.\nIn LSHTM\u0026rsquo;s Vaccine Centre (VaC), our world immunisation week (WIW) topic this year is \u0026ldquo;responding to outbreaks\u0026rdquo;. We want to highlight the collective action needed to protect people from outbreaks and vaccine-preventable diseases. This is the largest event series of the year, and we\u0026rsquo;ve spent several months meticulously planning activities to celebrate this week.\nAs a student liaison officer of VaC, I contributed in the following two projects - poster and podcast.\nPoster I used Inkscape to illustrate the key image for WIW - a little boy saying no to a virus, with a post-vaccinnation bandage and a VaC backpack. The blue theme is used in our daily newsletter and website deployment, in alignment with the school\u0026rsquo;s color policy. Podcast Serving as the student liaison officers of VaC, Ashna and I collaborated to record a podcast series with Ben Kasstan-Dabush. Ben is an Assistant Professor of medical anthropology at LSHTM, evaluating vaccine programme delivery amidst the COVID-19 pandemic, measle outbreaks, and poliovirus incidents within Jewish community. He has brilliant insights and critial thinking about vaccine engagement and the relationships that vaccine is embedded in. We had a excellent time interviewing him and gained a wealth of knowledge from both reading his research and engaging in conversation with him. Highly recommended!\n\u0026ldquo;Immunity is about relationships: outbreak response and vaccine engagement\u0026rdquo; Listen now for the trailer.\n\u0026ldquo;Immunity is about relationships: outbreak response and vaccine engagement\u0026rdquo; Listen now for the full episode.\nIn this episode, Assistant Professor Ben Kasstan-Dabush from the Department of Global Health \u0026amp; Development at LSHTM sits down with Student Liaison Officers Ashna Pillai and Hao Kai Tseng from the LSHTM Vaccine Centre to talk about vaccines and the relationships between parents, community, and health systems.\nBuilding on the fieldwork stories with Haredi Jewish families, Ben Kasstan-Dabush delves into his vaccine research around the religious communities and the complementary delivery pathways tailored for them.\nBen Kasstan-Dabush describes an outbreak as firefighting caused by a lack of investment. “How do we sustain that money, so that we\u0026rsquo;re not in a problem of constant firefighting.”, he said. Unfortunately, the learning and models implemented during the COVID-19 pandemic have limited transferability to routine immunisation programmes.\nTake the measles outbreak in the UK for instance, Ben Kasstan-Dabush discusses the concept of the link between the declining vaccine coverage and chronic deprivation. Thinking about “vaccine engagement” aids in understanding the complex relationships that vaccination is embedded in, where we should not lose sight of the particular political, economic, and social climate in which vaccines are delivered, alongside the ability and resources of our systems. Therefore, building resilience and flexibility into immunisation programmes is crucial for enhancing vaccine uptake.\nWhat do we need in outbreak management? How is deprivation linked with vaccine uptake? Why is it helpful to think about vaccine engagement? Together, Ben Kasstan-Dabush and the Student Liaison Officers will guide you through these questions, with a focus on the key lessons learned from COVID-19, measles, and poliovirus outbreaks.\nOther Events \u0026amp; Podcasts VaC hosts three events and other two podcasts during the WIW. Be sure not to miss them!\n🎈 EVENTS\n🗓 23 April | Vaccine Centre Annual Lecture: Professor Dame Sarah Gilbert Development of vaccines against outbreak pathogens\n🗓 25 April | Community engagement for vaccination in humanitarian settings: A view from the field\nIdaraobong Ekanem, ​IFRC / Nigeria Red Cross Sofi Lazlo, MSF Nada Abdelmagid, LSHTM Mohamed Kahow, ​Save the Children Somalia 🗓 30 April | Health promotion and vaccination in the age of social media: insights from the frontline\n​​Natalia Pasternak Taschner, University of São Paulo, Brazil​ Cherstyn Hurley, UKHSA Vishaal Virani, Head of UK Health, YouTube 📻 PODCASTS\n🗓 22 April | Responding to polio outbreaks – new vaccine, old foe\nEdward Parker, LSHTM Nick Grassly, Isobel Blake, and Laura Cooper, Imperial College London Ed Clarke and Dr Larry Kotei, MRC Unit in The Gambia at LSHTM 🗓 24 April | The use of vaccines in outbreak response\nEd Newman, Nadine Beckmann and Sophie Everest, UK Public Health Rapid Support Team Nicholas Davies, LSHTM ","date":"2024-04-22T22:11:12+08:00","permalink":"https://haokaitseng.github.io/post/20240425_world_immunisation_week/","section":"post","tags":null,"title":"World Immunisation Week 2024"},{"categories":["Data Science","Vaccine","Modelling"],"contents":" This group project is a collective effort, with contributions from my teammates Minn Thit Aung, Polly Nightingale, Simon Kent, and Xavier Dunn, listed alphabetically.\nThis post aims to:\nDetermine the the basic reproduction number (R0) of an hypothetical outbreak Assess the impact of various strategies for vaccination and school closures on accumulative cases and peak cases/timing Provide instructions for constructing a SEIR model using Berkeley Madonna Offer simple R code for converting a ggplot into a GIF Setting the Scene Many countries have recently experienced the first wave of an influenza pandemic caused by the strain HuNz. Now, there are rumours that in a distant country N, the second wave of the pandemic is just starting, and more infections have been reported so far.\nThe Ministry of Health in country P has become concerned and has called an emergency meeting to discuss the next steps, assuming that the transmissibility of the virus increases by 50% between the first and second waves.\nIf you were a group of modelling experts in the country P, how do you prepare for this emergency meeting and inform the Ministry of Health the decision-making process?\nModel Structure The type of model used is a deterministic model with an SEIR framework, short for Susceptible-Exposed-Infectious-Removed. This model uses differential equations to estimate the change in populations over time (rate) in the various compartments, assuming static patterns.\nNotably, we incorperate age-dependednt mixing, which means the transmission probability/parameter are different between age groups: children-to-children, adults-to-children, children-to-adults, and adults-to-adults.\nFitting the First Wave The information already known during the first wave includes: population (182,091 children and 369,909 adults), infectious period (2 days), latent period (2 days), fraction of symptom (60%), fraction of reporting (15%), vaccine efficacy (25%), susceptibility (50% first-wave infected individuals are fully protected against the second strain). Also, through the surveillance system, we\u0026rsquo;ve known the cummulative weekly number of reported infections for children and adults.\nWe must ascertain the basic reproductive number (R0) to inform our modelling for the second wave. Using the information mentioned, we apply the Curve Fit function in the Berkeley Madonna to fit our model to the first-wave data. Here comes the the optimal age-dependet transmission parameters: β1, β2, and β3 in the Who-Acquire-Infections-From-Whom (WAIFW) matrix. β is the per capita rate at which two individuals, in this case age-speceficly, come into effective contact per unit time.\nTo derive the R0 in the first wave, Next Generation Matrix is used along with a simulation process. Briefly, based on the initial assumed conditions, this method simulate the dynamics of the disease spread over time, i.e. from one generation to the next generation. Once the simulation has reached a steady state (equilibrium), we can estimate R0 using the observed data. Finally we get 1.47 as our R0 in the first wave, which is not far from the historical R0 of influenza in the 20s century.\nModelling for Second Wave No Intervention Assuming an initial case of one child infection (seeding) in our country and 100% fraction of reporting, we multiply the first-wave WAIFW matrix by 1.5 because the transmissibility of the second wave is believed to be 50% higher. The Berkeley Madonna SEIR model produces cruves of cumulative number of symptomatic cases of children, adults, and all population.\nThis is a no-intervention scenario (scenario 1). It is estimated that 28.7% children, 22.3% adults, and 24.4% total population will experience symptoms due to the new influenza strain.\nInterventions Due to a global shortage, we have no stocks of antivirals and hope that closing schools and vaccination will help limit the spread of the infection. However, the vaccines supplied for the second wave are limited and we are expected to receive enough doses for 50% of the population. The available vaccine doses will be given to children, with any remaining doses being given to adults.\nThe school-closure intervention will shut down the school for four weeks once the true proportion of children who have experienced symptoms during the second wave has reached 0.5%, which is called the school-closure threshold. We suppose that the β1 will be 60% lower during the school-closure period, i.e. the β1 need to be multiplied with 0.4.\nIn the global environment of the Berkeley Madonna model:\rb1_school_open = 5.83845e-6 b1_school_closure = 2.33538e-6\rb1 = if ((time\u0026gt; 53) and (time\u0026lt;83)) then b1_school_closure else b1_school_open where on the 54th day the true proportion of children who have experienced symptoms hit the threshold\rNow we can try the modelling for four basic scenrios:\nScenario 1: No intervention Scenario 2: School Closure Scenario 3: Vaccination for 100% children and 25.39% adults (=50% total population) Scenario 4: Vaccination and School Closure, scenario 2 and 3 combined Owing to interventions, the proportion symptomatic in the total population are 24.1%, 19.8%, and 19.4% in scenarios 2, 3, and 4, respectively. It can be observed that the impact of school closure is limited, compared to vaccination programme.\nIn terms of flatting the curve and delaying the peak, similarly, the vaccination programme performs better than school closure does. The most effective strategy for controlling the outbreak involves a combination of vaccination and school closure measures (scenario 4).\nThe R code for this animation is included in the appendix at the bottom.\nIt is worth examining the Net Reproductive Number (Rn) at various point of time, calculated as the product of the transmission rate (β) with the susceptible population and the duration of infectiousness using the next generation matrix. In the begining of the scenario 1\u0026amp;2, the Rn is 1.59. However, at the first day of the school closure in scenario 2, the Rn decreases to 1.155.\nWith the collective protection from vaccination and acquired immunity from the first wave, the Rn in the begining of the scenario 3\u0026amp;4 is 1.32. Upon the implementation of school closure in scenario 4, Rn becomes 1.164. The Rn reaches a critical value of one at the peak of each curve.\nSensitivity Analysis To explore the potential strategies in response to the outbreak, we come up with the following advanced scenarios:\nScenario 5: Longer School closure (3 months) Scenario 6a: Adjsut the threshold of school closure from 0.5% to 0.2%. Scenario 6b: Adjsut the threshold of school closure from 0.5% to 1%. Scenario 7: Equal vaccine distribution: distribute the same doses vaccines to children and adults. Scenario 8: The strain is 200% higher transmissible than the first wave Scenario 9: Adjust the fraction of reporting from 100% to 60% Vaccination Distribution: Equity or Equality? We regard scenario 3 as a equitable strategy for vaccine distribution, as it prioritize individuals who are more vulnerable to the disease and have a higher transmission rate (where β1 \u0026gt; β2 \u0026gt; β3).\nIn contrast, scenario 7 is a equal strategy for vaccine distribution. This is a equal allocation due to the same weight (number of vaccine doses) given to both groups of people.\nIt can be observed that an equitable vaccine distribution program proves more effective in controlling outbreaks compared to equal distribution models, this is due to the equity-driven resource allocation favoring people with higher transmission rates (β1).\nThresholds for School Closure Apart from proportion symptomatic, peak cases and peak timing are also critical indicators for controlling an outbreak.\nScenarios Peak Cases Peak Day Prop. Symp. 8-Vac\u0026amp;200%Trans 20,917 52 40% 7-Vac (equal) 9,597 65 29% 1-No interv. 6,077 87 24% 2-Clos.(0.5%) 5,298 113 24% 3-Vac 3,243 140 20% 6a-Vac\u0026amp;Clos.(0.2%) 3,077 155 20% 4-Vac\u0026amp;Clos.(0.5%) 2,839 156 19% 6b-Vac\u0026amp;Clos.(1%) 2,499 157 19% 9-Vac\u0026amp;60%Rep. 1,946 140 20% 5-Vac\u0026amp;Clos.(3m) 1,462 151 17% In this table, we sort the scenarios by the peak cases, and we focus on three scenarios with vaccination and different thresholds for school closure (scenario 6a, 4, and 6b). These three scenarios have similar peak timing and proportion symptomatic. However, the higher the threshold we set, the lower the peak cases we have.\nIn other words, as long as we know the surge capacity of the healthcare system, we can manipulate the threshold to flatten the curve, preventing the health care systems from overwhelming patients.\nConclusion Key Finding Choosing from four basic scenarios (1-4), we recommened the vaccination only strategy to our Ministry of Health to mitigate the surges in influenza cases.\nIn scenario 4, the combined effects of vaccination and school closure result in the largest reduction of symptomatic cases (20.6%) compared to scenario 1 as the reference. However, closing schools may delay the peak of cases, while the significance depends on whether burden to health services, given zero mortality in this context.\nCost to society of school closure may result in disrupting children\u0026rsquo;s education/causing parents to miss work. This highlights the importance of considering the Incremental Cost Effectiveness Ratio (ICER) in the modelling.\nLimitations This hypothetical study has some limitations due to the assumptions. First, no births or deaths are included in model. Second, we assume the latent and infectious periods in the second wave are the same as virus in first wave. Third, proportions of cases showing symptoms is assumed the same in adults and children, and the fraction of reporting is not possible to reach 100% in the realworld settings.\nRecommendations First of all, it\u0026rsquo;s important to collect more data to conduct a more comprehensive economic evaluation of control measures, which is the highlighted in a panel discussion at LSHTM - The use of modelling to inform decision-making in an emergency: lessons from COVID-19. Assessing changes in behavior throughout an outbreak in real-time can be challenging as they are difficult to predict in advance. Besides, assessing the effects on healthcare service is critial. It may also be useful to conduct a cohort/surveillance that we study for seroprevalence through outbreaks. The Ministry of Health may try to combine other non-pharmaceutical interventions, e.g. social distancing, wearing mask, personal hygiene.\nThis group project was originally intended for a group presentation of the the Modelling and the Dynamics of Infectious Diseases module at LSHTM, with contributions from Minn Thit Aung, Polly Nightingale, Simon Kent, and Xavier Dunn, listed alphabetically.\nAppendix # The gganimate package demonstrates how to make the ggplot in the scenario 1-4 into a GIF\r#The data structure should be: a day of newly reported cases in each row, and each column is a scenario\rlibrary(gganimate)\rplot_object \u0026lt;- ggplot() +\rgeom_line(data=df1,aes(x=Time,y=Scenario1),color=\u0026quot;#088199\u0026quot;,size=0.7)+\rgeom_line(data=df1,aes(x=Time,y=Scenario2),color=\u0026quot;purple\u0026quot;,size=0.7)+\rgeom_line(data=df1,aes(x=Time,y=Scenario3),color=\u0026quot;#Ca0020\u0026quot;,size=0.7)+\rgeom_line(data=df1,aes(x=Time,y=Scenario4),color=\u0026quot;#b8e186\u0026quot;,size=0.7)+\rgeom_text(data=df1,aes(x=Time, y=Scenario1 + 1.5,label=paste('S1')), color = \u0026quot;#088199\u0026quot;)+\rgeom_text(data=df1,aes(x=Time, y=Scenario2 + 1.5,label=paste('S2')), color = \u0026quot;purple\u0026quot;) +\rgeom_text(data=df1,aes(x=Time, y=Scenario3 + 1.5,label=paste('S3')), color = \u0026quot;#Ca0020\u0026quot;)+\rgeom_text(data=df1,aes(x=Time, y=Scenario4 + 1.5,label=paste('S4')), color = \u0026quot;#b8e186\u0026quot;)+\rtheme(panel.grid.major = element_blank(),\raxis.line = element_line(colour = \u0026quot;black\u0026quot;),\rpanel.background = element_rect(fill = \u0026quot;transparent\u0026quot;),\rplot.title=element_text(hjust=0.5)) +\rscale_x_continuous(breaks=seq(0,350,25))+\rlabs(y=\u0026quot;Daily New Reported Cases\u0026quot;,x = 'Time (Day)',title = \u0026quot;Outbreak Dynamics of Influenza Strain HuNz\u0026quot;)\rprint(plot_object)\rplot_object_transition \u0026lt;- plot_object + transition_reveal(Time) # make static into dynamic\ranimate(plot_object_transition, #save the GIF\rfps = 50, #frame rate\rduration = 1.5, #speed\rwidth = 1800, height = 865, #dimension\rrenderer = gifski_renderer(loop = FALSE,\u0026quot;My_animation.gif\u0026quot;),\rres= 100)# accuracy every fold\r","date":"2024-03-30T22:11:12+08:00","permalink":"https://haokaitseng.github.io/post/20240327_modelling_flu/","section":"post","tags":null,"title":"Equity vs. Equality: Modelling Vaccine Distribution Strategies for Outbreak Response Decision-Making"},{"categories":["Data Science"],"contents":" How do you apply time-to-event analysis to compare the impact of different prescriptions on death?\nThis article examines the survival function of two prescriptions using Kaplan-Meier and Cox models in an electronic health records (EHR) setting. EHR data are powerful real-world data. They are conducive to time-to-event analysis owing to the characteristic of sequential visits to primary and secondary care services. Take UK\u0026rsquo;s OpenSAFELY for instance, this secure, transparent, and open-source platform provides an Trusted Research Environment (TRE) for National Health Service (NHS) EHR data analysis, which supported urgent research into the COVID-19 emergency.\nSetting the Scene Building on a hypothetical EHR data, this project aims to understand the impact of a prescription of proton pump inhibitor (PPI) versus a prescription for a histamine H2-receptor antagonist (H2RA) on all-cause mortality. To extract analysis data from various datasets of EHR and code lists, I use the following eligible criteria to define the study cohort:\nPPI and H2RA prescription with first prescription timeframe from 17 April 1997 to 17 April 2017 the first prescription is after patient\u0026rsquo;s registration at general practice (GP) plus one year the first prescription is after the patient’s 18th birthday. The study follow-up begins at the first prescription for PPI or H2RA, and it ends at the first of:\ndeath (event of interest) transfer out of GP end of study (17 April 2017), i.e. administrative censoring Considering confounding effects, 10 potential confounding variables from the cohort are also examined: (1) age at prescription, (2) gender, (3) ethnicity, (4) Index of Multiple Deprivation (IMD), (5) body max index (BMI), (6) calendar year of first description, (7) diagnosis of gastric cancer prior to prescription, (8) gastro-oesophageal reflux disease (GERD) in the 6 months prior to prescription, (9) peptic ulcer in the 6 months prior to prescription, and (10) number of consultations in the year prior to prescription.\nNotably, addressing missing data is an essential part of EHR data analysis. I adopt a strategy where patients\u0026rsquo; birth dates are assumed to be on the 15th of June in the given birth year. Additionally, for BMI, I prioritize my-self-calculated BMI over those reported by GPs, while ensuring that the derived BMI values fall within an acceptable and realistic range of heights and weights.\nTime-to-event Data For the purpose of survival analysis, I define the length between index date (at first description) and end date (either death, censoring due to out of GP, or censoring due to end of study) as the days in the study.\nFrom the perspective of exposure, the median length of follow-up is 4 years in PPI group and 9 years in H2RA group, respectively. The distribution of the length of follow-up is strongly right-skewed in the PPI group, whereas such distribution is not observed in the H2RA group.\nLength of Follow-up by Prescription/Death in Years Subgroup N Median IQR PPI 12,984 9 4.5-13.4 H2RA 61,816 4 0.5-6.1 Death 13,741 3.4 0.3-5 Alive 61,059 5.2 0.8-8.6 Disaggregated by outcome, those who are alive in the cohort period have longer follow-up (median 5.2 years), in comparison of the median 3.4 years for those who died.\nKaplan-Meier Method To investigate the crude survival probability across two exposure groups, the non-parametric estimate of Kaplan-Meier is used. The probability of surviving 5 years or more is 0.888 (95% confidence interval, CI: 0.883-0.894) in H2RA group, whereas the probability of surviving 5 years or more is 0.796 (95% CI: 0.792-0.8) in PPI group.\nThe survivor functions in H2RA group are always higher than those in PPI group. As time goes by, the difference in survivor functions between the two groups increases, from 0.057 at year 1, to 0.109 at year 10, to 0.132 at year 19.\nBy using log-rank test, an overall lower survival probability in PPI group is confirmed (the chi-square statistic is 556 on 1 degree of freedom, p-value \u0026lt; 0.05), in comparison to H2RA group.\nCox Regression Univariable Model By fitting a univariable Cox proportional hazard model, a semi-parametric method, the log hazard ratio estimate is 0.52. The estimated hazard ratio is 1.68, which means the hazard for all cause death is increased by 68% in the PPI group relative to the H2RA group. The 95% CI (1.61-1.76), not covering null, indicates a strong evidence against the null hypothesis of no association between exposure and the hazard for death.\nNote that the Cox Proportional Hazard (PH) assumption implies the hazard ratio measuring the effect of any predictor, i.e. prescriptions in this case, is constant over time.\nMultivariable Model Considering potential confounding, I examine whether the effect estimate, e.g. odds ratios in logistic regression, changes substantially after adding each candidate confounding variable in the Cox model. Also, for the purpose of survival curve estimation, certain non-confounding varaibles are selected into the multivariable Cox proportional hazard model.\nFurthermore, to investigate whether the functional form for the continuous variable age is appropriate in the multivariable model, I calculate the Martingale residuals for age and number of consultations. However, it can be observed from the LOESS lines that direct employing a linear term for these continous varaible is appropriate.\nIn the final multivariable Cox PH model adjusting seven selected confounding ( below), the log hazard ratio estimate is 0.093. The estimated hazard ratio is 1.098 (95% CI: 1.049-1.150), meaning the hazard for all cause death is increased by 9.8% in the PPI group compared to the H2RA group. Compared to the univariable model, the effect of PPI is reduced in the multivariable model, yet still significant.\nh(t│X) = h0 (t)exp⁡{ β1*X{PPI}+β2*X{calendar period}+β3*X_{IMD person}+β4*X{gender} +β5*X_{ethnicity}+β6*X{recent GERD}+β7*X{number of consultations}+β8*X{age} Xi:exposure for individual i (i=1,…,n); ti:death or censoring time for individual i; δ:indicator of death (1)or censoring (0).\nAssessing the PH Assumption I use the scaled Schoenfeld residual to assess the assumption of the multivariable Cox PH model. The null hypothesis in this test is that the corresponding β(t) coefficient does not vary over time, indicating that the proportional hazards assumption holds for that variable.\nIt can be observed that the hazard ratio is changing over time in some variables. The global Chi-square statistic in the Schoenfeld residual is 211 (p-value \u0026lt; 0.05), indicating this model in general didn’t follow the proportional hazard assumptions. For instance, in the beginning of follow-up, PPI’s hazard is similar to H2RA’s hazard, but the log hazard ratio of PPI is lower at later times.\nExtended Cox Model Since the assumption of the Cox PH model is violated, I use extended Cox model to accommodate the non-proportionality. This approach allows the hazard ratios to be dependent on time, i.e. interacting with time. After splitting cohort data into five-year chunks according to the adjusted years of follow-up in the study, we can see the hazard ratios of PPI over H2RA in 4 periods.\nIn the extended model, the overall hazard ratio is 1.165 in PPI group relative to H2RA group. However, using PPI-period 1 (year 0-5) as reference in the extended Cox model, the estimate of hazard ratio is 0.946 in the PPI-period 2 (year 5-10) group compared to H2RA group during the same period, with other covariate holding constant.\nEstimating Survival Curve To apply the extended Cox model to hypothetical patient profiles, imagine four scenarios:\nA man aged 50 in the white ethnicity group, with no co-morbidities and in the most deprived group. A man aged 50 in the white ethnicity group, with recent diagnosis of Gastroesophageal reflux disease (GERD) prior to prescription and in the least deprived group. As in 1. but for a woman. As in 2. but for a woman. I plot the Cox survival curves in four scenarios by four periods, given this extended model is built upon time-dependent effect for the exposure. It can be observed that males (solid line) have lower survival functions compared to females (dashed line). In each scenarios during period 1, 2, and 3, the survival functions is lower in PPI group compared to H2RA group. However, in the period 4 (year 15 to 20), H2RA seems to have a higher risk of death, where its survival curve is under the PPI group.\nConclusion EHR data are often used for survival analysis, either in parametic (Weibull), non-parametric (Kaplan-Meier), or semi-parametric (Cox) model. This article examines the observations which are times at which death occurs, in comparison of two prescriptions. Based on the extended Cox regression, hazard ratio gets increased by 16.5% in PPI group relative to H2RA group, whereas the hazard ratio is changing over time.\nThis article has been adapted from the assessment of the Analysis of Electronic Health Record module at LSHTM.\n","date":"2024-03-23T22:11:12+08:00","permalink":"https://haokaitseng.github.io/post/20240322-survival-analysis/","section":"post","tags":null,"title":"Survival Analysis in Electronic Health Records Data"},{"categories":["Data Science","Machine Learning"],"contents":" In this article, we will explore the following topics:\nUsing regularized method (Lasso) for predictive variable selection Tuning hyperparameters for tree-based methods Employing the weighted sum of weak learners for boosted classifier Comparing prediction performances and predictors importance Basic Methods Inmagine you possess a dataset comprising 30 biomarker varaibles with 5000+. How would you use it to predict patient\u0026rsquo;s remission status, i.e. remission or active disease? One common approach that may cross your mind is the logistic regression, as illustrated below:\nlogit(p) = β0​+β1​X1​+β2​X2 ​+…+β30*​X30\nIn addition to logistic regression, we can apply t-test to inform the normalised difference between remission\u0026rsquo;s binary outcome.\nAlso, regarding collinearity, it worths looking into the correlations across the predictive variables. Through visual inspection, we can identify some strong correlations, such as alt \u0026amp; ast, ymph_percent * neut_percent in the correlations plot.\nRegularized Mothod Lasso regression is a regularized method in machin learning that performs both variable selection and regularization. It can identify the insignificant or unimportant variables as zero coefficients. The variable selection and shrinkage effect are strong with the penalty parameter λ. With the increase of λ in the below, the number of non-zero coefficients reduces from 30 to 0, where the lines are shrinkage paths.\nTo find an optimal hyperparameter λ, I used an automated 10-fold cross validation on Lasso. The lowest error is observed at the log λ of -8.11. However, I want to choose the λ at which error is within 1 standard error of the minimal error, i.e. the 1 se criterion. To balance the bias-variance trade-offs, log λ of -4.66 is preferred because it provides similar predictive performance compared to log λ of -8.11. This is also much easier to interpret with less variables, and less likely to overfit to noise in the training data.\nAfter tuning the λ, there are 12 no-zero beta coefficients remaining in the Lasso laerner. The complexity of the model is hence reduced.\nTree-based Methods CART As a supervised learning approach, a single Classification And Regression Trees (CART) is a useful algorithm, which is used as a predictive model to draw conclusions about a set of observations. At each node, the tree grows in a binary direction, initiating the first split based on whether the hgb is less than 13 or not. In the node where hgb \u0026lt; 13, it is calssified that 67% of the patients has a status of remission.\nBagging With more than one decision trees, our learning model can be become ensemble, which means the kearning process is made up of a set of classifiers. The bootstrap aggregation, also known as bagging, is the most well-known ensemble method. Using bagging, a random sample of data in the training set is selected with replacement and then trained independently. Finally, taking the average or majority of those estimates yield a more accurate prediction.\nIn our training data, I loop over 10 to 500 trees for bagging. This help me to determine the number of trees required to sufficiently stabilize the Root Mean Squared Error (RMSE). With 340 trees, the model achieves the lowest RMSE and demonstrates a tendency to stabilize thereafter.\nRandom forest Random forest algorithm is an extension of bagging, as it uses both bagging and feature randomness to create an uncorrelated forest of decision trees. In random forest, a selection of a subset of features ensures low correlation among decision trees, which can reduce over fitting and increase accuracy.\nTo training a random forest algorithm, five hyperparameters should be considered:\nSplitting rule Maximum tree depth/ Minimum node size Number of trees in the forest Sampling fraction Number of predictors to consider at any given split (mtry) To find an optimal mtry, here I utilize 10-fold cross validation for random search and grid search. In random search, when you have 8 predictors to consider at any given split, you have the highest accuracy. In grid search, the optimal mtry is 10, with a higher Kappa, indicating a better classifier, considering the marginal distribution of the remission status.\nWith regard to sampling fraction, reducing the sample size can help minimize between-tree correlation. As different sets of sampling fractions are tried, either with or without sampling replacement, the minimal out-of-bag (OOB) RMSE is observed at 90%. This suggests that optimizing predictions can be achieved by drawing 90% of observations for the training of each tree.\nGoing through these searches of hyperparameters with minimal OOB RMSE, the tuned random forest requires at least 340 trees to grow, with 10 mtry and 90% sampling fraction (with replacement). Gini impurity is adopted for the split rule, which is suitable for the scope of classification.\nmtry 10 10 8 8 Sampling fraction OOB RMSE (replacement) OOB RMSE (non-replacement) OOB RMSE (replacement) OOB RMSE (non-replacement) 100% 0.5175449 0.5165891 0.5139516 0.5237148 90% 0.5137112 0.5173061 0.5165891 0.5194512 60% 0.5192133 0.5144322 0.5192133 0.5170672 Overall, in evaluating prediction performance, it is evident that the tuned random forest exhibits the lowest RMSEs among the tree-based methods.\nLearner OOB misclassification error RMSE (validation set) Single CART - 0.58809 340 bagged trees 0.2641463 0.5058941 Random forest (default) 0.2661 0.5049165 Random forest (tuned) 0.2639 0.5058941 Here the importance of the predictors are presented. The feature with the highest importance in the random forest is lymph_percent.\nAdaboost Adaptive boosting, known as adaboost, is also an ensemble learning method that combines multiple weak learners sequentially to adjust the weights of training instances based on their classification accuracy. Adaboost is usually applied in binary classification, where misclassified instances are given higher weights. These weak models are generated sequentially to ensurethat the mistakes of previous models are learned by their successors.\nWith a step size of 0.1, my optimal number of boosting iterations are 323, remainining 19 predictors in the generalized linear model. This number indicates the best balance between model performance (error) and computational efficiency.\nModels Comparison The specificity, sensitivity, Positive Predictive Value (PPV), Negative Predictive Value (NPV) of the tuned random forest model are all higher than the Logistic, LASSO, and Adaboost in the validation set, i.e. 20% of the data that is not used for training. The Kappa of 48% as of random forest means a fair agreement.\nLeaner Accuracy Kappa Sensitivity Specificity PPV NPV Logistic Regression 69.27% 37.67% 78.11% 59.20% 68.57% 70.35% Lasso 68.77% 36.43% 80.33% 55.60% 67.34% 71.27% Random forest 74.41% 48.33% 79.59% 68.50% 74.22% 74.65% Adaboost 68.97% 36.91% 79.59% 56.87% 67.77% 70.98% We particularly investigate the area underneath the ROC (AUC) of Lasso and random forest. For random forest, the AUC of 0.808 suggests a reasonable ability to discriminate between the remission status, better than Lasso’s AUC of 0.741. Moreover, there are statistical difference (Z = 6.6253, P \u0026lt; 0.05).\nFinally, let\u0026rsquo;s dive into the importance of the predictors. There are 6 common predictors selected by Lasso and Random forest. Hbg, mch, lympch_percent are the most impactful among five methods, where mch hgb and lymph percent had large normalised differences in the previous T-test analysis. In the figure below, the underlined predictors that are unique to either Lasso or random forest exhibit a distinct pattern in the previous correlation pairs.This is a characteristic of Lasso penalty: it will typically pick only one of a group of correlated predictor variables to explain the variation in the outcome.\nConclusion Random forest (74.41%) has better accuracy on the prediction for the validation data, surpassing logistic regression, Lasso, adn Adaboost. Besides, regarding AUC, random forest (0.741) has a good ability to discriminate between the remission and active disease.\nIn terms of predictor importance, among the 12 predictors selected by Lasso learner, 6 of them are also selected in the top 12 predictors identified by the random forest, indicating their importance. Therefore, through using these important predictors, fitting a tuned random forest model on their biomarkers can support the predicting of remission status.\nIn collaboration with ChatGPT, this article has been adapted from the assessment of the Machine Learning module at LSHTM, with thanks to lecturers Pierre Masselot, Alex Lewin, and Sudhir Venkatesan.\n","date":"2024-02-10T22:11:12+08:00","permalink":"https://haokaitseng.github.io/post/20240210-machine-learning-lasso-random-forest/","section":"post","tags":null,"title":"Predicting Remission Status in Healthcare: A Comparative Analysis of Lasso, Random Forest, and Adaboost Machine Learning Models"},{"categories":["Data Science","R","PostgreSQL","ODK","ETL"],"contents":" What can you learn from this article?\nUnderstand the concepts of data linkage, especially deterministic linakge. Address linkage error in the conjunction of MIMIC III (served in a postgreSQL database) and ODK database. Employ R to design the Extract, Transform, and Load (ETL) pipeline. Use Quarto document to generate a report in PDF format. Concepts of Data Linkage In a data scientist\u0026rsquo;s typical day, the merge/join function is an inevitable task. Data quality must be secured through linkage within one dataset (\u0026lsquo;internal linkage\u0026rsquo;) or across multiple datasets.\nAccording to the OECD Glossary of Statistical Terms, data linakge means a merging that brings together information from two or more sources of data with the object of consolidating facts concerning an individual or an event that are not available in any separate record.\nIn situations of uncertainty, there might be an linkage error, i.e. misidentifying relationships that belong to the same entity. Notably, linkage error doesn\u0026rsquo;t adhere to an binomial distribution (0 or 1); rather, it resembles a spectrum ranging from high agreement to high disagreement, along with matching possibilities influenced by quality of matching variables.\nSource: James Doidge, 2023\nIn order to evaluate the performance of data linkage, we can build a matrix for the link-match classification. In theory, we wish our data to achieve a ~100% sensitivity (proportion of matches that are linked), or recall, while in parralel keeping the specificity (proportion of non-matches that are not linked) high as well. To achieve these goals, three methods for data linkage are widely used, including deterministic linakge, probalistic linkage, and machine learning.\nMatch (records from the same entity) Non-match (records from different entities) Link True match False match Non-link Missed match True non-match Source: Collaboration in Research and Methodology for Official Statistics, European Commission\nI will apply the deterministic linkage method in this article. A set of predetermined rules is used to identify patterns as links or non-links. For instance, the high degree of certainty/agreement required for deterministic linkage is achieved through a unique identifier for an entity, such as NHS number. This method may allow a small amount of preconceived typographical error; however, the limitations of this method lie in the event of lower quality matching variables or handling large numbers of matching variables.\nA scenario during COVID pandemic Let\u0026rsquo;s dive into a practical scenario where data linkage proves invaluable. Suppose you are a data analyst in a UK hospital, where data is captured in the MIMIC III database and ODK database. Your hospital management has asked you to generate an executive report synthesising recent patient data from MIMIC III, survey data from ODK, and national statistics for COVID-19. Besdies, to address surges in COVID-19 cases, you should be able to regularly generate this report, with potential changes to the prototype.\nWhat\u0026rsquo;s your strategy on this mission?\nDBI First, a database interface (DBI) is required to initialize the extract, transform, load (ETL) workflow. DBI package can help connect my R to database management systems (DBMS).\nAs MIMIC III is served in a PostgreSQL database, I chose RPostgres package to help me build the connection from PostgreSQL to R. Notably, RPostgres package can better capture the variables with timestmaps, comepared to the PostgreSQL package. A few lines of postgreSQL code are executed here to retrieve the specific data I need.\nI then use ruODK package to configure and download data from ODK Central to R.\nAs for national statistics, I use the download.file() function in base R to get a CSV file from the data.gov.uk.\nData Linkage Now, I are about to merge the data from MIMIC III and ODK for further cleaning, transforming, filtering, and analysis. Based on deterministic linkage method, I assume/regard the subject_id as our reliable agreement for matching between two datasets.\nAfter utilizing R\u0026rsquo;s merge() for an inner join, I want to examine the quaity of the data linkage by introducing two additional matching variables: age at admission and gender.\nLinkage error in age Substantial variation in differnce in ages is noted following the linkage. The distribution of age difference for 25% to 75% of patients shows a deviation of within one year, which is deemed acceptable owing to rounding. Nevertheless, noticeable errors are evident in the minimum (-39) and maximum (303) age differences.\nDifference in ages between MIMIC III and ODK:\nMin. Q1 Median Median Q3 Max. -39 -1 1 33.95 1 303 To address this issue, we can exclude observations that surpass the expected lifespan. Subsequently, consult the data provider to determine which data source is more reliable regarding age entry. You can either rely on a single dataset as the definitive age or eliminate observations with significant deviations in ages.\nLinkage error in gender Let\u0026rsquo;s move on to the gender variable. There seems a bit mismatches, such as \u0026ldquo;Female-Man\u0026rdquo; and \u0026ldquo;Male-Woman\u0026rdquo;. Again, you can decide either omit all observations with mismatches in genders, or rely on a single dataset as the definitive gender.\nComparison of gender categories between MIMIC III and ODK:\nMIMIC III / ODK Man Woman Trans Non-binary Female 6 2620 4 4 Male 2953 2 8 10 It is also noteworthy that individuals identifying as trans and non-binary are retained in my merged data frame. This inclusion is crucial as gender minority groups may feel uncomfortable when presented with a table offering only binary options for female and male. In other words, for me, they are not linkage error; instead, \u0026lsquo;it\u0026rsquo;s a defect in the gender variable of MIMIC III.\nAfter handling the linkage error, I then apply some exploratory data analysis (EDA) to privde summary satatistics for our patients. This data processing is not covered in this article.\nQuarto Finally, following data anaylsis, I use Quarto, the next generation of R Markdown released in 2022, to play as our publishing system.\nYou can smoothly do all the programming in a single qmd file for Quarto document. Alternatively, you can also save your output from your previous R scripts somewhere, and then open a new qmd file to read and print your outputs. Knitr::kable and kableExtra package is highly recommended for generating decent LaTeX tables in a PDF format.\nConclusion In short, investigating the matching variables may provide an indication of confidence in the link. In my scenario, I minimized data linkage error by initially agreeing on subject_id, and then stepwisely and partially agreeing on age and gender.\nAdditionally, processing data from various sources is achieved by using R to execute the ETL pipeline, ensuring a reproducible workflow for synthesizing a report as below.\nThis article has been adapted from the assessment of the Health Data Management module at LSHTM. With thanks from James Doidge who introduced the data linkage topic in the Thinking Lika A Health Data Scientist module.\n","date":"2024-01-06T22:11:12+08:00","permalink":"https://haokaitseng.github.io/post/20240101-data-linkage-etl-mimic/","section":"post","tags":null,"title":"Minimizing data linkage error in an ETL pipeline using R: an intersection of MIMIC III and ODK database"},{"categories":["Vaccine","HPV"],"contents":" What can you learn from this article? An experimental cohort study design for policy evaluation, where countries made the move from a two-dose to single-dose schedule for national HPV vaccination programmes since 2023.\nIntroduction Since the first licensing of the Human Papillomavirus (HPV) vaccine in 2006, evidence has been emerging showing that single-dose schedules provide comparable efficacy to the conditional regimens, i.e. two or three doses. In 2022, a review of World Health Organization (WHO) Strategic Advisory Group of Experts on Immunization (SAGE) concluded that a single-dose HPV vaccine delivers solid protection against HPV. Consequently, there have been 16 countries that adopt SAGE\u0026rsquo;s recommendation of single-dose schedule in their national immunization programmes in 2023, including the UK [Figure 1].\nThis study aims to assess the non-inferiority of the real-world impacts in single-dose HPV vaccination programmes with regard of herd immunity and paradigm shift. Method This research is designed as a prospective and closed cohort study with a 20-year follow-up.\nSource Population Age: adolescent females aged 12 to 13 in 2023-24 in 16 study countries, i.e. age eligible to their national HPV vaccine program. Inclusion: get at least one dose of HPV vaccine and undergo screening in the follow-up period. Exclusion: those who migrate out of the study countries. Exposure The final vaccination status, categorized into two and three doses of HPV vaccines, is considered as two distinct exposures. Single-dose HPV vaccination serves as the reference group (non-exposure). Outcome Regarding the disease progression [Figure 2], in the first decade, this study defines precancer as the preliminary outcome, including Cervical intraepithelial neoplasia (CIN) 2+ and Adenocarcinoma in situ (AIS).\nCervical cancer is investigated as the secondary outcome over the last decade. Three kinds of methods are used for screening testing, including pap smear test, liquid based cytology (LBC) test, and visual inspection with acetic acid (VIA). Data Collection Follow-up continues until the diagnosis of cervical cancer (or precancer), death, or the end of 2043. The rationale for 20-year follow-up is the highest incidence rates being in the 30 to 34 age group in the UK, that will be the end of their follow-up.\nData integration is streamlined through annual data linkage across five sources from each country, including population immunization registry, cervical screening registry, death registry, national migration record, and regional socio-economic status.\nStatistical Analysis The demographic characteristics are depicted through the mean (± standard deviation) and counts (percentage).\nThis study uses Cox proportional-hazard regression, adjusted for sexual orientation and socio-economic index, with age as the time axis to estimate hazard ratios (with 95% Confidence Intervals, CIs) of cervical cancer (or precancer) stratified by country according to their vaccine doses.\nNon-inferiority is defined when the difference in hazard ratios is less than a 5% deviation from 1.\nRESULT (HYPOTHETICAL) Take UK for instance, the cohort include 305,307 vaccinated females. Of these, 259,511 (85%) received single dose, 39,690 (13%) received two doese, and 6,106 (2%) received three doses. Their mean age completed vaccination is 12.6, 13.8, and 15.3, respectively.\nOverall, the average number of screens is more than 2. The years between complete vaccination and screening, sexual orentation, and socio-economic index are detailed in Table 1.\nThe difference in the incidence rate ratio of three subgroups is negligible (1 dose 6.84, 2 doses 6.51, and 3 doses 6.42, per 100,000 person-years). The adjusted hazard ratio was not significantly lower for 2+ dose groups compared to single-dose women (2 doses 0.96 (95% CI 0.87-1.05) and 3 doses 0.95 (0.86-1.05)), indicating the non-inferiority of single-dose schedule [Table 2].\nOf 16 study countries, 14 single-dose HPV vaccination programmes achieve non-inferiority. (The findings from countries outside the UK are omitted.) DISSCUSION Strengths This research exhibits four strengths. First, the substantial sample size enhances the generalizability of findings. Second, Geographical comparisons are attainable thanks to multi-country data sources. Third, data are reliably obtained, as they are routinely updated in country\u0026rsquo;s immunization and screening programs. Last but not least, the final disease outcome, i.e. cervical cancer, is used as the study measurement, surpassing previous studies reliant on short-term outcomes, such as HPV persistent infection.\nLimitations and bias However, this research is constrained by its assumptions of not stratifying results according to vaccine brands or screening methods, the later may lead to misclassification due to diverse sensitivity and specificity of the testing. Unregistered or incorrect record of doses may also cause non-differential misclassification of the exposure. Such information bias is towards the null.\nWith regard to selection bias, the willingness of screening may be influenced by the final vaccination status. Notably, the exclusion of females who have not been vaccinated in the defined source population is essential to prevent such selection bias.\nTrade-offs When it comes to inferiority, this study\u0026rsquo;s emphasis lies in the trade-offs aspect. On the side of single dose, higher immunization coverage can be obtained, which is conducive to herd immunity. Besides, the number of cases averted will be more appealing to policy makers from a prospective of public health. Not to mention the benefit of efficient implementation and reduced costs.\nOn the side of 2+ doses, the relative risks exhibit a somewhat lower magnitude. With regard of the waning immunogenicity, some modelling studies suggest that some of the single dose girls need a catch-up dose to obtain sufficient immunity in population. CONCLUSION (HYPOTHETICAL) Single-dose HPV vaccination demonstrates comparable efficacy compared to 2+ doses in 14 out of the 16 study countries\u0026rsquo; programmes, suggesting that a shift to single-dose schedule is suitable for cervical cancer prevention.\nThis article has been adapted from the poster, originally created as part of the assessment for the epidemiology module at LSHTM. I express sincere gratitude to Professor Neil Pearce for his exceptional teaching.\n","date":"2023-12-15T22:11:12+08:00","permalink":"https://haokaitseng.github.io/post/20231216-hpv-single-dose-vaccination/","section":"post","tags":null,"title":"Assessing the Non-inferiority of the Single-dose Human Papillomavirus Vaccination Schedule: A Hypothetical Multi-country Cohort Study"},{"categories":["Data Science","Data governance"],"contents":" This article delivers two key insights:\nAdvantages of applying UK GDPR in research planning. Five approaches to safeguard data protection in lung cancer patient interviews. Setting th Scene The need of assessing the Quality of Life (QoL) in patients with lung cancer undergoing chemotherapy has been increasing. After treatment, patients may experience breathlessness or fatigue, along with potential challenges in their daily and occupational functioning. These side effects of chemotherapy consistently rank as a common complaint among patients. By using the validated QoL questionnaire from the European Organisation for Research and Treatment of Cancer, a hypothetical research project aim to summarize the QoL state among a cohort of patients in a hospital in the UK. Hopefully it can benefit the treatment of choice.\nHowever, concerns regarding data protection were brought up in a management meeting, with the focus on data confidentiality.\nMany patients consenting to participate in the study have uncertainty about their personal data processing, particularly in relation to sensitive questions. How do you lawfully and comprehensively tackle the data protection issue in a patient survey?\nData Governance in GDPR To manage such risks, the GDPR can be one of the best solutions that is inetegrated into the research planning and implementations. Implementing this information governance rules can yield numerous benefits, encompassing aspects such as safeguarding individual rights and freedom, fulfilling lawful obligations, and gaining hospital\u0026rsquo;s competitive advantages.\nConversely, non-compliance with GDPR may cause research failure, distress to individuals, fines and penalties, and damage to hospital\u0026rsquo;s reputation.\nGiven GDPR’s seven key principles below, I propose five approaches that center on the principle of integrity and confidentiality to mitigate information governance risks and remove doubts regarding data security. Procedures and policy for data processing security With regard to security in data processing, transfer, and storage, it can be achieved through technical procedures and organisational policy, such as encryption, pseudonymization, access control, and audit.\nInformation technology (IT) security remediation Through employing tools for reviewing IT security, system vulnerabilities can be identified and mitigated, along with fortified protection against potential security threats and cyber security events.\nProtocols for notifying a personal data breach Protocols need to be developed to respond to a personal data breach event, ensuring that the investigation and reporting are completed within 72 hours.\nAppointing a data protection officer This role’s responsibility is to oversee, monitor, and document operations related to data processing, applying extensive knowledge of data protection practices.\nA centralised area designated for data storage and processing To minimize the risk of data leak, data processing is exclusively and physically performed on these dedicated desktops in an office.\nFor more detailed you can refer to the GDPR’s Data Protection and Impact Assessment Table in the end of this post\nRecommended Approaches To balance between risks presented by processing personal data and research utility, I propose endorsement of the first three approaches as a selection of strategies for this study due to following reasons. First, they follow GDPR’s principle of integrity and confidentiality, a priority particularly valued by your patients.\nSecond, the advantages inherent in these approaches are substantial. For instance, the first and second approach can minimize the likelihood of negative events, such as unauthorized access or data breach; the third approach then serves as a response to such negative events.\nThird, the consequences of not implementing these approaches would be intolerable when weighed against their disadvantages, since a lack of robust safeguards for data protection can result in confidentiality risks, violation to regulations, and erode trust among patients.\nLastly, the estimated residual level of information governance risks (impact and probability) for the first three approaches are lower compare to the remaining approaches. A data protection officer (fourth approach) is considered more suitable for a large-scale study; a centralized area for data processing (fifth approach) runs a risk of accidental data corruption.\nConclusion Research planning can incorporate GDPR to help make trade-offs in data/information governance risks. While each GDPR’s principle is equally important, I recommend three approaches that can strengthen the compliance to the principle of integrity and confidentiality. By establishing procedures and policy, implementing IT security remediation, and generating protocols for notification, this hypothetical study can have appropriate safeguards in place to process data safely.\nFurthermore, I believe the assessment of QoL can provide the opportunity to improve QoL and symptom burden management of lung cancer patients with chemotherapy drugs. Therefore, we can not only accelerate research progress but also address confidentiality and regulatory requirements in our scope of research.\nData Protection and Impact Assessment Table What approach could be taken to reduce risk? What actions would this involve? What are the advantages and disadvantages of these actions for research? Approach 1: Procedures and policy for data processing security (1) Data processing involves the practice of protecting information from unauthorized access and corruption during the entire research lifecycle. (2)It encompasses robust encryption and pseudonymization of personal data, while in parallel ensuring confidentiality, integrity, and resilience of processing systems. (3)Regular audit and monitoring are required to promptly detect and address any anomalies or potential data breaches. (4)Encrypted data transfer is implemented, and data storage and network security are ensured through essential measures on physical hardware. (1) Advantages: (1-1)Implementing security procedures and policies helps mitigate the risks of accidental or unlawful destruction, alteration, loss, and unauthorized access. (1-2)It not only complies with Article 32 of the GDPR but also adheres to the accountability principle of the GDPR, as the security logs and documentation can be used to demonstrate compliance. (2) Disadvantages: Enforcing access controls could introduce complexity and potentially decrease efficiency within the research project. Approach 2: IT security remediation (1)IT security remediation is an actionable set by detecting and protecting against potential security vulnerabilities or cyber security events, such as unauthorized access or disclosure due to non-encrypted transmission of data. (2)It identifies existing and scheduled mitigations for security issue, and rank likelihood of the issue occurring given existing or scheduled mitigations. (3)It updates existing security and antivirus software and strengthens IT infrastructure to ensure mitigations and controls are in place. (1) Advantages: (1-1)Cybersecurity remediation can help manage risks and intercept IT security threats. This approach aligns with the National Cyber Security Centre (NCSC) Cyber Assessment Framework (CAF) guidance. (1-2)By identifying security concerns, it indirectly adheres to the integrity and confidentiality principle of the GDPR. (2)Disadvantages: Procurement and management of security software entails ongoing expenses. Approach 3: Protocols for notifying a personal data breach (1)In response to personal data breach, protocols and internal procedures should be well-planned, including pulling the facts, contain the breach, access the risk, and report it without delay, and act to protect affected patients. (2)After having become aware of personal data breach, the research team should notify the issue to the Information Commissioner, no later than 72 hours. (3)LSHTM’s Research Ethics Committees should also be notified when any breach of the study protocol or the principles of Good Clinical Practice has been identified. (1) Advantages: (1-1)According to Article 33 of the GDPR, reporting a personal data breach in a timely manner (\u0026lt;72 hours) can avoid fine (up to £8.7 million or 2 per cent of turnover) and penalties. (1-2)It meets the principle of lawfulness, fairness and transparency, accountability, and integrity and confidentiality in the GDPR. (1-3)A comprehensive procedure can save time to respond to an event of personal data breach and communication strategy (Article 34 of GDPR). (2)Disadvantage: Documented protocols and procedures are needed, along with essential training sessions for the research team. Approach 4: Appointing a data protection officer (1)The data protection officer is designated on the basis of data protection laws (GDPR) and practices for confidentiality. (2)The data protection officer is involved in all issues which relate to the protection of personal data, including regular and systematic monitoring of data subjects. (3)The data protection officer manages and maintains records of data analysis logs, as well as input and output activities within the designated area. (1)Advantages: (1-1)Compliance with regulations is regularly and systematically monitored, which aligns with the Article 37 of the GDPR and the principle of integrity and confidentiality and accountability of GDPR. (1-2)There is a contact point on issues relating to processing and protecting personal data. (2)Disadvantages: (2-1)The scope of this research is insufficient to warrant the role of a data protection officer, since systematic monitoring of data subjects is not conducted on a large scale. (2-2) Research budgetary allocation will be required to hire a data protection officer. Approach 5: A centralised area designated for data storage and processing (1)A structured set of personal data, including electronic health records, is stored in the filing system of the desktops in the information management department at the hospital. (2)Only members from the research have access to the research eligible patients’ personal data. (3)Data processing and analysis are exclusively and physically performed on these dedicated desktops. (1)Advantages: (1-1)It ensures that data remains confined to these systems and cannot be exported, minimizing the risk of data leak. (1-2)It provides protection against unauthorised processing of personal data, which aligns with the principle of integrity and confidentiality from the Article 5 of the GDPR. (2)Disadvantages: (2-1)There is a risk of accidental loss or corruption in the event of destruction or damage to the area of centralized data. (2-2)Remote work is not available. In collaboration with ChatGPT, this article has been adapted from the assessment of the Thinking like a Health Data Scientist module at LSHTM.\n","date":"2023-10-30T22:11:12+08:00","permalink":"https://haokaitseng.github.io/post/20231030-data-governance/","section":"post","tags":null,"title":"Optimizing Data Protection: Leveraging Information Governance Principles from GDPR into Research Planning"},{"categories":["Data Science","Python"],"contents":"This year, London home rental prices hit record as demand outstriped supply, with the average tenant now being asked to pay a whopping £2,500 a month for a new let.\nWithin this rental market, individuals seeking short-term accommodations encounter greater challenges. My roommates and I was looking for a house/flat for co-renting with a 6-month tenancy, and we put in a significant effort throughout the entire month of August by searching and making inquiries about over 170 properties before finally settling down. Using Python in Visual Studio Code, I hereby analyze the data in our list of properties, which might provide an insight for your search and budget.\nWhat you will learn in this article:\nTrade-off between comunicating time and rent. Key factors that determine rent. Strategies to find your property. Photo by João Barbosa on Unsplash\nTrade-off Between Comunicating Time and Rent We browsed websites such as OpenRent, RightMove, Spotahome, and Zoopla during our search. We documented the features of 174 properties, capturing details including price per calendar month, tenancy, number of bedrooms, number of bathrooms, maximum number of tenants, location (zone), and comunicating time to my School (London School of Hygiene and Tropical Medicine) measured using Google Maps.\nFirst, the majority of rental properties in the market typically require a minimum one-year tenancy commitment, whereas our preference is for a shorter-term tenancy of more or less 6 months, with the option to extend if everything proceeds smoothly. Hence, among the targeted properties that specified their tenancy requirements, 81% state their acceptance of a minimum 6-month tenancy.\nHere I provide a overview of our data by price, communicating time, and the number of bedrooms. The range of rent is between £1,850 to £4,500, while the range of communicating time is from 17 to 67 minutes.\nIf you are also sensitive to both cost and time, you can observe a noticeable pattern through simple linear regression. It suggests that you need to invest approximately £19 in order to gain an additional minute of time saved on your commute to school.\nTo validate the assumption of normality and homoscedasticity in the regression model, I make QQ-plot and random clouds. They seem pretty good.\nI also use K-means clustering algorithm to find the most suitable number of clusters according to rental costs. There is a substantial disparity between the outcome obtained using the elbow method and the Silhouette Score. The elbow method indicates that 4 clusters would be optimal, aligning somewhat with the established property zones. In contrast, the Silhouette Score suggests for a more fine-grained segmentation into 9 groups.\nKey Factors That Determine Rent Aside from communicating time, many factors also contribute to our living qualities. When conducting a multiple linear regression analysis, I identify that several variables significantly impact rent:\nProperty Zone: Choosing to reside in Zone 2 instead of Zone 1 can result in a savings of £319, while opting for Zone 3 over Zone 2 can lead to a savings of £102. Number of bedrooms: Acquiring an additional bedroom is associated with an increased cost of £384. Number of bathrooms: An additional bathroom comes with an added expense of £62. On the other hand, certain factors appear to have less influence on rent, including directional location of the property, maximum number of tenants, and minimum tenancy.\nStrategies to Find Your Property When seeking short-term accommodation in London it\u0026rsquo;s imperative to dedicate substantial effort to locate and secure your perfect property within this highly competitive rental market. In the following sections, I will outline three useful strategies that can enhance your search and help you save valuable time:\n1. Establish Your Financial Parameters and Objectives Begin by determining your budget constraints and the anticipated duration of your stay. Identify essential amenities that align with your preferences, such as a two-bathroom layout, laundry facilities, access to a rear garden, or parking availability. Additionally, be sure to specify your desired move-in date.\nIf your plan involves a shorter tenancy, be prepared for the possibility of residing further away from the center of London. Generally, properties locate in Zone 3 or the outskirts of London are more amenable to short-term tenancies. In addition, many landlords may require you to make an upfront payment of the total rent, if you lack a job, a reliable financial history, or references.\n2. Utilize Appropriate Online Platforms Consider exploring channels such as Spotahome, OpenRent, and various Facebook groups for your search. Spotahome operates in a manner similar to Airbnb, offering a comprehensive online process prior to your arrival. OpenRent may have a little landlords open to short-term tenancies, especially those looking to rent out properties located in remote or less competitive areas.\nHowever, exercise caution in Facebook rental groups, as scams can be a concern.\nMainstream websites like RightMove often feature properties gear toward long-term rentals, so they may not align with your short-term needs.\nLastly, don\u0026rsquo;t hesitate to contact a local agency to seek their assistance in locating a suitable property. Nestify is a good choice, since all of their properties are available for 3-9 months.\n3. Act Swiftly In the fast-paced environment, it\u0026rsquo;s essential to act swiftly to secure your desired house. You should regularly check rental websites that I mentioned for new listings everyday. Hot properties get inundated with inquiries. Send a detailed introduction to the landlord or agency.\nOnce a viewing is confirmed, whether in-person or virtually, make your decision promptly, I mean, in hours! Stay proactive throughout the process of paying deposit and signing contract. Delaying the process could result in the property being offered to other interested parties.\nConclusion I exemplify the know-how for searching for short-term accommodation London. Drawing on evidence-based analytics, this article offers data-driven insights into several factors influencing the cost of short-term rentals.\nNavigating the complex world of real estate in London can be daunting, but this article aims to anchor readers in their decision-making process.\nWishing you the very best of luck in your search!\nThis article is also published on Medium。\n","date":"2023-09-09T22:11:12+08:00","permalink":"https://haokaitseng.github.io/post/20230907london-rent-cost/","section":"post","tags":null,"title":"Navigating Short-Term Accommodation Search in London"},{"categories":["Essay"],"contents":"你知道，台灣是全球受境外假訊息侵擾最嚴重的國家，而且已經蟬聯榜首10年嗎？\n本文選台灣的生化實驗室為例，揭露一個境外資訊操弄與干預的手法。\n謠言的前世 從2020年COVID-19疫情以來，在當時的Twitter(現稱X)渠道上，就流傳一個謠言是，美國在全世界設的生化實驗室被陸續曝光，包含台灣的10座實驗室詳細名稱、地址、聯絡資訊等。\n但這訊息已有Cofacts真的假的和MyGoPen闢謠，指出「美國在台設實驗室做生化武器或研究大規模流行病」屬於陰謀論，企圖煽動與分化。\n台灣這則謠言的前身，是來自俄烏戰爭的假訊息。\n俄國曾指控美國在烏克蘭開設生化實驗室，俄烏戰爭開打後，還緊急銷毀致命的病原體。但實情是，美國在戰前就推動在烏克蘭的公衛及生物威脅減少計畫，並非研發生物武器，且銷毀只是避免實驗樣本落入俄軍手中。\n可見這次的陰謀論版圖擴大，把俄烏戰爭的謠言巨網張到台灣的上空。\n謠言的今生 今年7月，聯合報獨家報導美方要台灣開設P4實驗室研發生物戰劑。前立委雷倩亦在中天新聞的節目上表示，因為台灣人可以代表全中國人的DNA，所以美方在此研發生物戰劑是危險的。\n縱使國防部、國安局、外交部、美國在台協會立刻聲明此傳聞非事實，台灣曾簽署「禁止發展、製造、儲存、取得或保有生物及毒素武器並避免其可能引發之毀滅公約」，故不研發生物戰劑，且台北地檢署後續於9月5日認定所謂「南海工作會議紀錄」實屬造假，包括使用了許多中國式用語等。但雷倩的疑美片段被轉傳再製，最再加上微博大V玉渊谭天(實為中國央廣旗下的自媒體品牌)的推波助瀾，讓聲量達微博的135萬影片觀看人次：\n「真正被犧牲的到底是誰？」 ── 玉渊谭天《独家披露台湾生物实验室分布》(2022/8/16)\n玉渊谭天不但暗指台灣人民就是美國與台灣政府勾連下最大的犧牲者(因為台灣P2等級以上實驗室多在人口稠密區儲藏病毒)，還對中國人民恫嚇台灣病毒的威脅(台灣實驗室多位於靠近中國的西半部)，還附上繪聲繪影的證據(國防部預醫所增加的預算是為了建P4實驗室，以及預醫所疑似動土前後的空照圖)。陰謀論、疑美論、與誇張描述的手法可見一斑。\n在Facebook的繁體字社群，也發現一些異常操作。周曉彤轉貼來自Tiktok的短影片，爆料美國要在台灣建造P4實驗室，但其帳號本身可疑，因其大頭貼顯為盜用；其發文還被李倩(帳號同樣不真)分享到15個反綠陣營社團，內容完全複製相同，顯非一般自然人的行為模式(Pattern of Behavior)。 根據上述資料，可以看出境外資訊操弄與干預(FIMI, Foreign Information Manipulation Interference)的痕跡，其中串聯了議題、行動者、動機(利益)、以及行動四大要素。雖然微博大V還透過標籤劫持(Hashtag Hijacking)來吸引注意，但這事件在台灣主流社群平台上聲量相對不高(至多在中天新聞上傳於Youtube的節目片段有5萬人次觀看)，討論度熱度也不持久。\n有趣的是，今年8月美國疾病管制單位在加州破獲一間非法生物實驗室，其中有實驗老鼠、化學藥品及帶有傳染源的生物材料。這起非法實驗室疑與中國有關，引爆美國的國安危機，同時亦可做為「台灣生化實驗室」謠言的真實對造組。\n結語 透過SCOTCH框架調查發現，兩岸媒體以及非國家行動者使用Youtube、Facebook、X(Twitter)、微博、Tiktok等社群平台，引用台媒的獨家報導及台灣的實驗室相關圖資，通過標籤劫持來吸引注意，以及在有共同理念(Shared-Interest)的Facebook社團中複製貼上分享貼文。 試圖在中國群眾及台灣反對民進黨的群眾中，植入美方籲台建造P4實驗室研發生物戰劑的謠言，這謠言帶有部分真實性，但滲入不實資訊或過度解讀，以植入民進黨政府犧牲民眾安全的疑美論，不但破壞台灣民眾對國家生物安全政策的信任，也令中國群眾感受到來自台灣的生化威脅。\n本文使用的工具及分析方法感謝台灣民主實驗室的課程。\n本文同步發表於Medium。\n","date":"2023-08-31T22:11:12+08:00","permalink":"https://haokaitseng.github.io/post/20230901%E5%8F%B0%E7%81%A3%E7%94%9F%E5%8C%96%E5%AF%A6%E9%A9%97%E5%AE%A4/","section":"post","tags":null,"title":"有關台灣生化實驗室的境外資訊操弄與干預 / A Case Study of Foreign Information Manipulation Interference: United States Asked Taiwan to Develop Weaponized Biological Agents in P4 Lab"},{"categories":["Essay"],"contents":"1. 全球總人口當中，有3.5%的人是移工。但在COVID-19(下稱新冠肺炎)確診數最高的20國當中，移工佔其總人口數至少4.5%，這些移工在疫情當中相對地面臨更高的感染風險以及生存考驗。\n早於疫情之前，移工容易成為社會中的弱勢族群。但疫情使得這群人更為脆弱，原因包含邊境的封鎖、產業緊縮導致的失業或勞動條件劣化、集體式(易群聚感染)的住所、對健康資訊的語言識讀障礙、身分變動造成在社會安全網邊緣化或排除、還有對外來者的歧視等等。\n我們可以從跨境匯款的消長來想像移工經濟影響的程度。根據世界銀行在去年四月的推估，2020年匯往中低收入國家的款項估計較原先預估金額大減20%，雖然在後來10月的報告中有對此比例下調為11%，但這幅度還是相當驚人，代表許多移工出外打拼，卻可能因為疫情失業，賺不夠錢寄回老家供養。但有些不尋常的金流，例如尼泊爾在下半年度收到較往年更多的匯款，推測與當地疫情更趨嚴重，海外家人加強金援有關。\n受影響的還有國際照顧鍊的末端──已開發國家的勞動力需求。日本目前因全面鎖國，營造業與服務業的人力短缺立刻浮現；韓國的農工等也面臨短缺，尤其像季節性移工完全無法入境，反倒讓現有的農工日薪漲了兩三成。\n於是報導常用stranded一詞，來形容在疫情中大規模擱淺的移工，不論是在人身自由、收入、或思鄉的心情上。\n2. 但在臺灣──一個好一段時間被新冠病毒遺忘的島嶼，這詞蘊含的景況全然不同。\n指揮中心表示，案〇〇〇為印尼籍20多歲男性，今(2020)年〇〇月〇〇日來臺工作(〇〇〇〇)，持有登機前3日內核酸檢驗陰性報告，入境後至防疫旅館進行居家檢疫，迄今無症狀；〇〇月〇日檢疫期滿後，搭乘專車至其他處所自主健康管理，〇〇月〇〇日由公司安排自費採檢，於今日確診(Ct值〇〇)。衛生單位已匡列個案接觸者共〇人，因有適當防護，列為自主健康管理。\n這是我一個疫調到半夜的個案，最後在新聞稿上呈現的段落。不為人知的是，那些列為自主健康管理的接觸者，與個案同一時期入境，一起搭車採檢，跟個案也語言不通，很快就被外商公司遣送回國了。當時我連忙跟仲介解釋，他們有著入境陰性+14天檢疫+7天自主自康管理+口罩+自費陰性的金牌，我們防疫並沒有需要做到這個地步，有其他例如再次自費的辦法來解決…。「公司很謹慎」，仲介只能無奈地回應。\n相較於接觸者是不被停留的人，這位印尼男孩則是徹底地被滯留了。受過大學教育的他，來臺灣工作是他出社會後第一份合約，檢疫/管理約20天後，再接一個月的住院隔離，中途歷經一度陰性卻陽轉的絕望。最終指揮官核定解隔後，他表示想回家(公司也不願用他)，但過了數週仍登不上飛機，因為搭機需要的核酸陰性報告，他出院驗了兩次還是生不出來(輕症者可能痊癒後數個月仍驗得出病毒核酸)。\n他因為邊境管制政策沒有隨新知調整，就這樣被持續被軟禁在旅館，此刻還在等待我們與印尼官方的協商結果。另一個協調成功的案例是，一位確診移工二採陰出院後，需要做外籍移工入境健檢，卻被多家醫院拒絕，最後有賴我老闆出面才有醫院願意幫她做健檢。\n對於仲介而言，移工縱使有搭機前陰性報告，還是與危險劃上等號，檢疫完還得公費/自費採檢才能放心；再次採檢的移工就是金雞母，仲介會追問我們報告結果，才能趕快交工抽成；但一旦陽性，移工就成了掃把星，仲介會被追殺做疫調(因為語言不通需仰賴翻譯老師)，好一點仲介的會在寒流時關心同鄉移工穿得夠不夠暖，差一點的仲介就不聞不問，或拒絕移工在住院期間想要的額外飲食花費，甚至有移工被仲介罵哭，說「妳為什麼要帶病毒來臺灣」。\n「(確診)這樣誰敢用？」這常是仲介接到噩耗當下的第一反應。\n從1992年頒布\u0026lt;就業服務法\u0026gt;，臺灣引進產業與社福類移工快滿30年，但因為活動領域顯有交集，70萬移工對許多本地人而言是隱形或(因為聽不懂)刺耳的存在。\n是疫情讓潛在30年的歧視浮現與變質。從去年年底因印尼移工佔境外移入的大宗，指揮中心宣布全面暫停引進印尼籍移工，疫情新聞下方就總會有要移工滾回母國、別帶病毒來台、健保不該補助醫療費用等仇外留言。我們也常接到民眾陳情哪裡有居檢的移工亂跑，或某公眾場所有移工群聚，或說移工對社區的老人小孩很危險云云。\n另一方面，疫情也讓移工的脆弱性有所反轉。因為許多航班停飛、減班、或漲價，聘僱期滿逾期居留的外籍移工無法遣送離境，導致收容中心被擠爆，再加上收容替代措施，非法移工便可以暫時合法地打黑工，黑市薪資水漲船高。而國際照顧鍊的斷層，讓有長照需求的家庭有錢也找無工，引發其他社會問題。\n3. 在傳統傳染病領域，藍領移工本就是政府嚴加監控的對象。像我們部門需花多達三分之一的人力，以高額罰鍰輔導仲介記得多次的移工定期健檢，來篩檢出寄生蟲、結核病等境外移入傳染病；現因應新冠肺炎疫情，疾管署有製作多國語言素材的防疫宣導品，還與勞動部啟動泰、越籍移工檢疫期滿篩檢等加強措施。\n但這樣就足夠了嗎？私認為一旦桃園守不住，社區開始流行，兩類移工可能成為傳播加速器：一是住團體宿舍的產業類移工，二是無合法身分的移工。\n新加坡就是第一類移工成為疫情破口的借鑑。去年4月爆發的大規模移工宿舍群聚感染，就是因為宿舍環境長期壅擠不人性化所導致；他們最終在去年12月以核酸與血清檢測大量篩檢發現，移工族群的新冠肺炎盛行率高達47%(相當於1萬5千人確診)，對比一般市民僅4千多的確診數，可為懸殊。但危機也可以是轉機，新加坡也因為疫情開始重視外籍移工的居住空間，調整使宿舍房間大小與密度符合國際勞動組織的規範。\n第二類移工曾在去年疫病濫觴時成為焦點。一位非法的移工看護在醫院染疫，引發相關部門加強查緝的動作，但最後指揮官因防疫量能的考量，反對掃蕩非法看護而作罷。\n上周才剛接到一通雇主說剛檢疫完的移工逃跑的電話，問這樣會不會造成防疫漏洞？我只能說我不知道。但我們更應該關心的是移工為什麼會逃跑的結構問題，這顯現我們修訂諸多勞動政策──取消移工三年出國一日、零付費改革、直聘等，仍無法根除仲介體制的剝削。唯有透過讓非法移工(無論是逃跑或聘期滿回不去)取得合法身分，才能促使移工重新被管理，願意通報與檢疫。\n你願不願意告訴我你生體不舒服，且不顧慮坦白之後的後果呢？\n一個防疫體制的成功與否，就在於它對其所有成員的可信賴性。病毒宿主並不分種族膚色，因此這信賴網絡文化的維護，不僅是勞動單位、仲介、雇主他們的事，這也是我們的事。\n本文亦同步發表於Medium\n","date":"2021-02-01T22:11:12+08:00","permalink":"https://haokaitseng.github.io/post/20210201%E7%96%AB%E6%83%85%E4%B8%8B%E7%9A%84%E7%A7%BB%E5%B7%A5-%E6%B5%81%E9%9B%A2%E5%B0%8B%E5%B2%B8%E7%9A%84%E4%BA%BA/","section":"post","tags":null,"title":"疫情下的移工，流離尋岸的人 / Migrant Workers Are Displaced Under COVID-19 Pandemic"}]