[{"categories":["Data Science"],"contents":" How do you apply time-to-event analysis to compare the impact of different prescriptions on death?\nThis article examines the survival function of two prescriptions using Kaplan-Meier and Cox models in an electronic health records (EHR) setting. EHR data are powerful real-world data. They are conducive to time-to-event analysis owing to the characteristic of sequential visits to primary and secondary care services. Take UK\u0026rsquo;s OpenSAFELY for instance, this secure, transparent, and open-source platform provides an Trusted Research Environment (TRE) for National Health Service (NHS) EHR data analysis, which supported urgent research into the COVID-19 emergency.\nSetting the Scene Building on a hypothetical EHR data, this project aims to understand the impact of a prescription of proton pump inhibitor (PPI) versus a prescription for a histamine H2-receptor antagonist (H2RA) on all-cause mortality. To extract analysis data from various datasets of EHR and code lists, I use the following eligible criteria to define the study cohort:\nPPI and H2RA prescription with first prescription timeframe from 17 April 1997 to 17 April 2017 the first prescription is after patient\u0026rsquo;s registration at general practice (GP) plus one year the first prescription is after the patient’s 18th birthday. The study follow-up begins at the first prescription for PPI or H2RA, and it ends at the first of:\ndeath (event of interest) transfer out of GP end of study (17 April 2017), i.e. administrative censoring Considering confounding effects, 10 potential confounding variables from the cohort are also examined: (1) age at prescription, (2) gender, (3) ethnicity, (4) Index of Multiple Deprivation (IMD), (5) body max index (BMI), (6) calendar year of first description, (7) diagnosis of gastric cancer prior to prescription, (8) gastro-oesophageal reflux disease (GERD) in the 6 months prior to prescription, (9) peptic ulcer in the 6 months prior to prescription, and (10) number of consultations in the year prior to prescription.\nNotably, addressing missing data is an essential part of EHR data analysis. I adopt a strategy where patients\u0026rsquo; birth dates are assumed to be on the 15th of June in the given birth year. Additionally, for BMI, I prioritize my-self-calculated BMI over those reported by GPs, while ensuring that the derived BMI values fall within an acceptable and realistic range of heights and weights.\nTime-to-event Data For the purpose of survival analysis, I define the length between index date (at first description) and end date (either death, censoring due to out of GP, or censoring due to end of study) as the days in the study.\nFrom the perspective of exposure, the median length of follow-up is 4 years in PPI group and 9 years in H2RA group, respectively. The distribution of the length of follow-up is strongly right-skewed in the PPI group, whereas such distribution is not observed in the H2RA group.\nLength of Follow-up by Prescription/Death in Years Subgroup N Median IQR PPI 12,984 9 4.5-13.4 H2RA 61,816 4 0.5-6.1 Death 13,741 3.4 0.3-5 Alive 61,059 5.2 0.8-8.6 Disaggregated by outcome, those who are alive in the cohort period have longer follow-up (median 5.2 years), in comparison of the median 3.4 years for those who died.\nKaplan-Meier Method To investigate the crude survival probability across two exposure groups, the non-parametric estimate of Kaplan-Meier is used. The probability of surviving 5 years or more is 0.888 (95% confidence interval, CI: 0.883-0.894) in H2RA group, whereas the probability of surviving 5 years or more is 0.796 (95% CI: 0.792-0.8) in PPI group.\nThe survivor functions in H2RA group are always higher than those in PPI group. As time goes by, the difference in survivor functions between the two groups increases, from 0.057 at year 1, to 0.109 at year 10, to 0.132 at year 19.\nBy using log-rank test, an overall lower survival probability in PPI group is confirmed (the chi-square statistic is 556 on 1 degree of freedom, p-value \u0026lt; 0.05), in comparison to H2RA group.\nCox Regression Univariable Model By fitting a univariable Cox proportional hazard model, a semi-parametric method, the log hazard ratio estimate is 0.52. The estimated hazard ratio is 1.68, which means the hazard for all cause death is increased by 68% in the PPI group relative to the H2RA group. The 95% CI (1.61-1.76), not covering null, indicates a strong evidence against the null hypothesis of no association between exposure and the hazard for death.\nNote that the Cox Proportional Hazard (PH) assumption implies the hazard ratio measuring the effect of any predictor, i.e. prescriptions in this case, is constant over time.\nMultivariable Model Considering potential confounding, I examine whether the effect estimate, e.g. odds ratios in logistic regression, changes substantially after adding each candidate confounding variable in the Cox model. Also, for the purpose of survival curve estimation, certain non-confounding varaibles are selected into the multivariable Cox proportional hazard model.\nFurthermore, to investigate whether the functional form for the continuous variable age is appropriate in the multivariable model, I calculate the Martingale residuals for age and number of consultations. However, it can be observed from the LOESS lines that direct employing a linear term for these continous varaible is appropriate.\nIn the final multivariable Cox PH model adjusting seven selected confounding ( below), the log hazard ratio estimate is 0.093. The estimated hazard ratio is 1.098 (95% CI: 1.049-1.150), meaning the hazard for all cause death is increased by 9.8% in the PPI group compared to the H2RA group. Compared to the univariable model, the effect of PPI is reduced in the multivariable model, yet still significant.\nh(t│X) = h0 (t)exp⁡{ β1*X{PPI}+β2*X{calendar period}+β3*X_{IMD person}+β4*X{gender} +β5*X_{ethnicity}+β6*X{recent GERD}+β7*X{number of consultations}+β8*X{age} Xi:exposure for individual i (i=1,…,n); ti:death or censoring time for individual i; δ:indicator of death (1)or censoring (0).\nAssessing the PH Assumption I use the scaled Schoenfeld residual to assess the assumption of the multivariable Cox PH model. The null hypothesis in this test is that the corresponding β(t) coefficient does not vary over time, indicating that the proportional hazards assumption holds for that variable.\nIt can be observed that the hazard ratio is changing over time in some variables. The global Chi-square statistic in the Schoenfeld residual is 211 (p-value \u0026lt; 0.05), indicating this model in general didn’t follow the proportional hazard assumptions. For instance, in the beginning of follow-up, PPI’s hazard is similar to H2RA’s hazard, but the log hazard ratio of PPI is lower at later times.\nExtended Cox Model Since the assumption of the Cox PH model is violated, I use extended Cox model to accommodate the non-proportionality. This approach allows the hazard ratios to be dependent on time, i.e. interacting with time. After splitting cohort data into five-year chunks according to the adjusted years of follow-up in the study, we can see the hazard ratios of PPI over H2RA in 4 periods.\nIn the extended model, the overall hazard ratio is 1.165 in PPI group relative to H2RA group. However, using PPI-period 1 (year 0-5) as reference in the extended Cox model, the estimate of hazard ratio is 0.946 in the PPI-period 2 (year 5-10) group compared to H2RA group during the same period, with other covariate holding constant.\nEstimating Survival Curve To apply the extended Cox model to hypothetical patient profiles, imagine four scenarios:\nA man aged 50 in the white ethnicity group, with no co-morbidities and in the most deprived group. A man aged 50 in the white ethnicity group, with recent diagnosis of Gastroesophageal reflux disease (GERD) prior to prescription and in the least deprived group. As in 1. but for a woman. As in 2. but for a woman. I plot the Cox survival curves in four scenarios by four periods, given this extended model is built upon time-dependent effect for the exposure. It can be observed that males (solid line) have lower survival functions compared to females (dashed line). In each scenarios during period 1, 2, and 3, the survival functions is lower in PPI group compared to H2RA group. However, in the period 4 (year 15 to 20), H2RA seems to have a higher risk of death, where its survival curve is under the PPI group.\nConclusion EHR data are often used for survival analysis, either in parametic (Weibull), non-parametric (Kaplan-Meier), or semi-parametric (Cox) model. This article examines the observations which are times at which death occurs, in comparison of two prescriptions. Based on the extended Cox regression, hazard ratio gets increased by 16.5% in PPI group relative to H2RA group, whereas the hazard ratio is changing over time.\nThis article has been adapted from the assessment of the Analysis of Electronic Health Record module at LSHTM.\n","date":"2024-03-23T22:11:12+08:00","permalink":"https://haokaitseng.github.io/post/20240322-survival-analysis/","section":"post","tags":null,"title":"Survival Analysis in Electronic Health Records Data"},{"categories":["Data Science","Machine Learning"],"contents":" In this article, we will explore the following topics:\nUsing regularized method (Lasso) for predictive variable selection Tuning hyperparameters for tree-based methods Employing the weighted sum of weak learners for boosted classifier Comparing prediction performances and predictors importance Basic Methods Inmagine you possess a dataset comprising 30 biomarker varaibles with 5000+. How would you use it to predict patient\u0026rsquo;s remission status, i.e. remission or active disease? One common approach that may cross your mind is the logistic regression, as illustrated below:\nlogit(p) = β0​+β1​X1​+β2​X2 ​+…+β30*​X30\nIn addition to logistic regression, we can apply t-test to inform the normalised difference between remission\u0026rsquo;s binary outcome.\nAlso, regarding collinearity, it worths looking into the correlations across the predictive variables. Through visual inspection, we can identify some strong correlations, such as alt \u0026amp; ast, ymph_percent * neut_percent in the correlations plot.\nRegularized Mothod Lasso regression is a regularized method in machin learning that performs both variable selection and regularization. It can identify the insignificant or unimportant variables as zero coefficients. The variable selection and shrinkage effect are strong with the penalty parameter λ. With the increase of λ in the below, the number of non-zero coefficients reduces from 30 to 0, where the lines are shrinkage paths.\nTo find an optimal hyperparameter λ, I used an automated 10-fold cross validation on Lasso. The lowest error is observed at the log λ of -8.11. However, I want to choose the λ at which error is within 1 standard error of the minimal error, i.e. the 1 se criterion. To balance the bias-variance trade-offs, log λ of -4.66 is preferred because it provides similar predictive performance compared to log λ of -8.11. This is also much easier to interpret with less variables, and less likely to overfit to noise in the training data.\nAfter tuning the λ, there are 12 no-zero beta coefficients remaining in the Lasso laerner. The complexity of the model is hence reduced.\nTree-based Methods CART As a supervised learning approach, a single Classification And Regression Trees (CART) is a useful algorithm, which is used as a predictive model to draw conclusions about a set of observations. At each node, the tree grows in a binary direction, initiating the first split based on whether the hgb is less than 13 or not. In the node where hgb \u0026lt; 13, it is calssified that 67% of the patients has a status of remission.\nBagging With more than one decision trees, our learning model can be become ensemble, which means the kearning process is made up of a set of classifiers. The bootstrap aggregation, also known as bagging, is the most well-known ensemble method. Using bagging, a random sample of data in the training set is selected with replacement and then trained independently. Finally, taking the average or majority of those estimates yield a more accurate prediction.\nIn our training data, I loop over 10 to 500 trees for bagging. This help me to determine the number of trees required to sufficiently stabilize the Root Mean Squared Error (RMSE). With 340 trees, the model achieves the lowest RMSE and demonstrates a tendency to stabilize thereafter.\nRandom forest Random forest algorithm is an extension of bagging, as it uses both bagging and feature randomness to create an uncorrelated forest of decision trees. In random forest, a selection of a subset of features ensures low correlation among decision trees, which can reduce over fitting and increase accuracy.\nTo training a random forest algorithm, five hyperparameters should be considered:\nSplitting rule Maximum tree depth/ Minimum node size Number of trees in the forest Sampling fraction Number of predictors to consider at any given split (mtry) To find an optimal mtry, here I utilize 10-fold cross validation for random search and grid search. In random search, when you have 8 predictors to consider at any given split, you have the highest accuracy. In grid search, the optimal mtry is 10, with a higher Kappa, indicating a better classifier, considering the marginal distribution of the remission status.\nWith regard to sampling fraction, reducing the sample size can help minimize between-tree correlation. As different sets of sampling fractions are tried, either with or without sampling replacement, the minimal out-of-bag (OOB) RMSE is observed at 90%. This suggests that optimizing predictions can be achieved by drawing 90% of observations for the training of each tree.\nGoing through these searches of hyperparameters with minimal OOB RMSE, the tuned random forest requires at least 340 trees to grow, with 10 mtry and 90% sampling fraction (with replacement). Gini impurity is adopted for the split rule, which is suitable for the scope of classification.\nmtry 10 10 8 8 Sampling fraction OOB RMSE (replacement) OOB RMSE (non-replacement) OOB RMSE (replacement) OOB RMSE (non-replacement) 100% 0.5175449 0.5165891 0.5139516 0.5237148 90% 0.5137112 0.5173061 0.5165891 0.5194512 60% 0.5192133 0.5144322 0.5192133 0.5170672 Overall, in evaluating prediction performance, it is evident that the tuned random forest exhibits the lowest RMSEs among the tree-based methods.\nLearner OOB misclassification error RMSE (validation set) Single CART - 0.58809 340 bagged trees 0.2641463 0.5058941 Random forest (default) 0.2661 0.5049165 Random forest (tuned) 0.2639 0.5058941 Here the importance of the predictors are presented. The feature with the highest importance in the random forest is lymph_percent.\nAdaboost Adaptive boosting, known as adaboost, is also an ensemble learning method that combines multiple weak learners sequentially to adjust the weights of training instances based on their classification accuracy. Adaboost is usually applied in binary classification, where misclassified instances are given higher weights. These weak models are generated sequentially to ensurethat the mistakes of previous models are learned by their successors.\nWith a step size of 0.1, my optimal number of boosting iterations are 323, remainining 19 predictors in the generalized linear model. This number indicates the best balance between model performance (error) and computational efficiency.\nModels Comparison The specificity, sensitivity, Positive Predictive Value (PPV), Negative Predictive Value (NPV) of the tuned random forest model are all higher than the Logistic, LASSO, and Adaboost in the validation set, i.e. 20% of the data that is not used for training. The Kappa of 48% as of random forest means a fair agreement.\nLeaner Accuracy Kappa Sensitivity Specificity PPV NPV Logistic Regression 69.27% 37.67% 78.11% 59.20% 68.57% 70.35% Lasso 68.77% 36.43% 80.33% 55.60% 67.34% 71.27% Random forest 74.41% 48.33% 79.59% 68.50% 74.22% 74.65% Adaboost 68.97% 36.91% 79.59% 56.87% 67.77% 70.98% We particularly investigate the area underneath the ROC (AUC) of Lasso and random forest. For random forest, the AUC of 0.808 suggests a reasonable ability to discriminate between the remission status, better than Lasso’s AUC of 0.741. Moreover, there are statistical difference (Z = 6.6253, P \u0026lt; 0.05).\nFinally, let\u0026rsquo;s dive into the importance of the predictors. There are 6 common predictors selected by Lasso and Random forest. Hbg, mch, lympch_percent are the most impactful among five methods, where mch hgb and lymph percent had large normalised differences in the previous T-test analysis. In the figure below, the underlined predictors that are unique to either Lasso or random forest exhibit a distinct pattern in the previous correlation pairs.This is a characteristic of Lasso penalty: it will typically pick only one of a group of correlated predictor variables to explain the variation in the outcome.\nConclusion Random forest (74.41%) has better accuracy on the prediction for the validation data, surpassing logistic regression, Lasso, adn Adaboost. Besides, regarding AUC, random forest (0.741) has a good ability to discriminate between the remission and active disease.\nIn terms of predictor importance, among the 12 predictors selected by Lasso learner, 6 of them are also selected in the top 12 predictors identified by the random forest, indicating their importance. Therefore, through using these important predictors, fitting a tuned random forest model on their biomarkers can support the predicting of remission status.\nIn collaboration with ChatGPT, this article has been adapted from the assessment of the Machine Learning module at LSHTM, with thanks to lecturers Pierre Masselot, Alex Lewin, and Sudhir Venkatesan.\n","date":"2024-02-10T22:11:12+08:00","permalink":"https://haokaitseng.github.io/post/20240210-machine-learning-lasso-random-forest/","section":"post","tags":null,"title":"Predicting Remission Status in Healthcare: A Comparative Analysis of Lasso, Random Forest, and Adaboost Machine Learning Models"},{"categories":["Data Science"],"contents":" What can you learn from this article?\nUnderstand the concepts of data linkage, especially deterministic linakge. Address linkage error in the conjunction of MIMIC III (served in a postgreSQL database) and ODK database. Employ R to design the Extract, Transform, and Load (ETL) pipeline. Use Quarto document to generate a report in PDF format. Concepts of Data Linkage In a data scientist\u0026rsquo;s typical day, the merge/join function is an inevitable task. Data quality must be secured through linkage within one dataset (\u0026lsquo;internal linkage\u0026rsquo;) or across multiple datasets.\nAccording to the OECD Glossary of Statistical Terms, data linakge means a merging that brings together information from two or more sources of data with the object of consolidating facts concerning an individual or an event that are not available in any separate record.\nIn situations of uncertainty, there might be an linkage error, i.e. misidentifying relationships that belong to the same entity. Notably, linkage error doesn\u0026rsquo;t adhere to an binomial distribution (0 or 1); rather, it resembles a spectrum ranging from high agreement to high disagreement, along with matching possibilities influenced by quality of matching variables.\nSource: James Doidge, 2023\nIn order to evaluate the performance of data linkage, we can build a matrix for the link-match classification. In theory, we wish our data to achieve a ~100% sensitivity (proportion of matches that are linked), or recall, while in parralel keeping the specificity (proportion of non-matches that are not linked) high as well. To achieve these goals, three methods for data linkage are widely used, including deterministic linakge, probalistic linkage, and machine learning.\nMatch (records from the same entity) Non-match (records from different entities) Link True match False match Non-link Missed match True non-match Source: Collaboration in Research and Methodology for Official Statistics, European Commission\nI will apply the deterministic linkage method in this article. A set of predetermined rules is used to identify patterns as links or non-links. For instance, the high degree of certainty/agreement required for deterministic linkage is achieved through a unique identifier for an entity, such as NHS number. This method may allow a small amount of preconceived typographical error; however, the limitations of this method lie in the event of lower quality matching variables or handling large numbers of matching variables.\nA scenario during COVID pandemic Let\u0026rsquo;s dive into a practical scenario where data linkage proves invaluable. Suppose you are a data analyst in a UK hospital, where data is captured in the MIMIC III database and ODK database. Your hospital management has asked you to generate an executive report synthesising recent patient data from MIMIC III, survey data from ODK, and national statistics for COVID-19. Besdies, to address surges in COVID-19 cases, you should be able to regularly generate this report, with potential changes to the prototype.\nWhat\u0026rsquo;s your strategy on this mission?\nDBI First, a database interface (DBI) is required to initialize the extract, transform, load (ETL) workflow. DBI package can help connect my R to database management systems (DBMS).\nAs MIMIC III is served in a PostgreSQL database, I chose RPostgres package to help me build the connection from PostgreSQL to R. Notably, RPostgres package can better capture the variables with timestmaps, comepared to the PostgreSQL package. A few lines of postgreSQL code are executed here to retrieve the specific data I need.\nI then use ruODK package to configure and download data from ODK Central to R.\nAs for national statistics, I use the download.file() function in base R to get a CSV file from the data.gov.uk.\nData Linkage Now, I are about to merge the data from MIMIC III and ODK for further cleaning, transforming, filtering, and analysis. Based on deterministic linkage method, I assume/regard the subject_id as our reliable agreement for matching between two datasets.\nAfter utilizing R\u0026rsquo;s merge() for an inner join, I want to examine the quaity of the data linkage by introducing two additional matching variables: age at admission and gender.\nLinkage error in age Substantial variation in differnce in ages is noted following the linkage. The distribution of age difference for 25% to 75% of patients shows a deviation of within one year, which is deemed acceptable owing to rounding. Nevertheless, noticeable errors are evident in the minimum (-39) and maximum (303) age differences.\nDifference in ages between MIMIC III and ODK:\nMin. Q1 Median Median Q3 Max. -39 -1 1 33.95 1 303 To address this issue, we can exclude observations that surpass the expected lifespan. Subsequently, consult the data provider to determine which data source is more reliable regarding age entry. You can either rely on a single dataset as the definitive age or eliminate observations with significant deviations in ages.\nLinkage error in gender Let\u0026rsquo;s move on to the gender variable. There seems a bit mismatches, such as \u0026ldquo;Female-Man\u0026rdquo; and \u0026ldquo;Male-Woman\u0026rdquo;. Again, you can decide either omit all observations with mismatches in genders, or rely on a single dataset as the definitive gender.\nComparison of gender categories between MIMIC III and ODK:\nMIMIC III / ODK Man Woman Trans Non-binary Female 6 2620 4 4 Male 2953 2 8 10 It is also noteworthy that individuals identifying as trans and non-binary are retained in my merged data frame. This inclusion is crucial as gender minority groups may feel uncomfortable when presented with a table offering only binary options for female and male. In other words, for me, they are not linkage error; instead, \u0026lsquo;it\u0026rsquo;s a defect in the gender variable of MIMIC III.\nAfter handling the linkage error, I then apply some exploratory data analysis (EDA) to privde summary satatistics for our patients. This data processing is not covered in this article.\nQuarto Finally, following data anaylsis, I use Quarto, the next generation of R Markdown released in 2022, to play as our publishing system.\nYou can smoothly do all the programming in a single qmd file for Quarto document. Alternatively, you can also save your output from your previous R scripts somewhere, and then open a new qmd file to read and print your outputs. Knitr::kable and kableExtra package is highly recommended for generating decent LaTeX tables in a PDF format.\nConclusion In short, investigating the matching variables may provide an indication of confidence in the link. In my scenario, I minimized data linkage error by initially agreeing on subject_id, and then stepwisely and partially agreeing on age and gender.\nAdditionally, processing data from various sources is achieved by using R to execute the ETL pipeline, ensuring a reproducible workflow for synthesizing a report as below.\nThis article has been adapted from the assessment of the Health Data Management module at LSHTM. With thanks from James Doidge who introduced the data linkage topic in the Thinking Lika A Health Data Scientist module.\n","date":"2024-01-06T22:11:12+08:00","permalink":"https://haokaitseng.github.io/post/20240101-data-linkage-etl-mimic/","section":"post","tags":null,"title":"Minimizing data linkage error in an ETL pipeline using R: an intersection of MIMIC III and ODK database"},{"categories":null,"contents":"Hi! My name is Hao Kai Tseng. I am currently a graduate student of Health Data Science Programme at the London School of Hygiene \u0026amp; Tropical Medicine (LSHTM).\nI acquired my BSc Public Health from the National Taiwan University in 2016. I have gained experience in the field of infectious diseases through my work in both a research institute and a governmental health sector Taiwan.\nI am interested in policy implementation and evaluation related to infectious diseases. The COVID-19 pandemic has heightened my focus on strengthening society\u0026rsquo;s capacity for pandemic prevention, preparedness, and response. Besides, I am actively engaged in assessing TB patient costs for WHO’s End Tuberculosis Strategy.\nAs a Student Liaison Officer of the Vaccine Centre at LSHTM, my career aspirations revolve around streamlining data pipelines in support of immunization initiatives.\nAs a gay, promoting equity is my mission.\nIn an attempt to pull my skillsets to the cloud, this blog aims to offer practical applications and inspirations that give the push you need to explore the realms of public health and data science.\nI wish you good luck in your search!\n","date":"2023-12-21T20:18:54+03:00","permalink":"https://haokaitseng.github.io/about/","section":"","tags":null,"title":"About Me"},{"categories":["Vaccine"],"contents":" What can you learn from this article? An experimental cohort study design for policy evaluation, where countries made the move from a two-dose to single-dose schedule for national HPV vaccination programmes since 2023.\nIntroduction Since the first licensing of the Human Papillomavirus (HPV) vaccine in 2006, evidence has been emerging showing that single-dose schedules provide comparable efficacy to the conditional regimens, i.e. two or three doses. In 2022, a review of World Health Organization (WHO) Strategic Advisory Group of Experts on Immunization (SAGE) concluded that a single-dose HPV vaccine delivers solid protection against HPV. Consequently, there have been 16 countries that adopt SAGE\u0026rsquo;s recommendation of single-dose schedule in their national immunization programmes in 2023, including the UK [Figure 1].\nThis study aims to assess the non-inferiority of the real-world impacts in single-dose HPV vaccination programmes with regard of herd immunity and paradigm shift. Method This research is designed as a prospective and closed cohort study with a 20-year follow-up.\nSource Population Age: adolescent females aged 12 to 13 in 2023-24 in 16 study countries, i.e. age eligible to their national HPV vaccine program. Inclusion: get at least one dose of HPV vaccine and undergo screening in the follow-up period. Exclusion: those who migrate out of the study countries. Exposure The final vaccination status, categorized into two and three doses of HPV vaccines, is considered as two distinct exposures. Single-dose HPV vaccination serves as the reference group (non-exposure). Outcome Regarding the disease progression [Figure 2], in the first decade, this study defines precancer as the preliminary outcome, including Cervical intraepithelial neoplasia (CIN) 2+ and Adenocarcinoma in situ (AIS).\nCervical cancer is investigated as the secondary outcome over the last decade. Three kinds of methods are used for screening testing, including pap smear test, liquid based cytology (LBC) test, and visual inspection with acetic acid (VIA). Data Collection Follow-up continues until the diagnosis of cervical cancer (or precancer), death, or the end of 2043. The rationale for 20-year follow-up is the highest incidence rates being in the 30 to 34 age group in the UK, that will be the end of their follow-up.\nData integration is streamlined through annual data linkage across five sources from each country, including population immunization registry, cervical screening registry, death registry, national migration record, and regional socio-economic status.\nStatistical Analysis The demographic characteristics are depicted through the mean (± standard deviation) and counts (percentage).\nThis study uses Cox proportional-hazard regression, adjusted for sexual orientation and socio-economic index, with age as the time axis to estimate hazard ratios (with 95% Confidence Intervals, CIs) of cervical cancer (or precancer) stratified by country according to their vaccine doses.\nNon-inferiority is defined when the difference in hazard ratios is less than a 5% deviation from 1.\nRESULT (HYPOTHETICAL) Take UK for instance, the cohort include 305,307 vaccinated females. Of these, 259,511 (85%) received single dose, 39,690 (13%) received two doese, and 6,106 (2%) received three doses. Their mean age completed vaccination is 12.6, 13.8, and 15.3, respectively.\nOverall, the average number of screens is more than 2. The years between complete vaccination and screening, sexual orentation, and socio-economic index are detailed in Table 1.\nThe difference in the incidence rate ratio of three subgroups is negligible (1 dose 6.84, 2 doses 6.51, and 3 doses 6.42, per 100,000 person-years). The adjusted hazard ratio was not significantly lower for 2+ dose groups compared to single-dose women (2 doses 0.96 (95% CI 0.87-1.05) and 3 doses 0.95 (0.86-1.05)), indicating the non-inferiority of single-dose schedule [Table 2].\nOf 16 study countries, 14 single-dose HPV vaccination programmes achieve non-inferiority. (The findings from countries outside the UK are omitted.) DISSCUSION Strengths This research exhibits four strengths. First, the substantial sample size enhances the generalizability of findings. Second, Geographical comparisons are attainable thanks to multi-country data sources. Third, data are reliably obtained, as they are routinely updated in country\u0026rsquo;s immunization and screening programs. Last but not least, the final disease outcome, i.e. cervical cancer, is used as the study measurement, surpassing previous studies reliant on short-term outcomes, such as HPV persistent infection.\nLimitations and bias However, this research is constrained by its assumptions of not stratifying results according to vaccine brands or screening methods, the later may lead to misclassification due to diverse sensitivity and specificity of the testing. Unregistered or incorrect record of doses may also cause non-differential misclassification of the exposure. Such information bias is towards the null.\nWith regard to selection bias, the willingness of screening may be influenced by the final vaccination status. Notably, the exclusion of females who have not been vaccinated in the defined source population is essential to prevent such selection bias.\nTrade-offs When it comes to inferiority, this study\u0026rsquo;s emphasis lies in the trade-offs aspect. On the side of single dose, higher immunization coverage can be obtained, which is conducive to herd immunity. Besides, the number of cases averted will be more appealing to policy makers from a prospective of public health. Not to mention the benefit of efficient implementation and reduced costs.\nOn the side of 2+ doses, the relative risks exhibit a somewhat lower magnitude. With regard of the waning immunogenicity, some modelling studies suggest that some of the single dose girls need a catch-up dose to obtain sufficient immunity in population. CONCLUSION (HYPOTHETICAL) Single-dose HPV vaccination demonstrates comparable efficacy compared to 2+ doses in 14 out of the 16 study countries\u0026rsquo; programmes, suggesting that a shift to single-dose schedule is suitable for cervical cancer prevention.\nThis article has been adapted from the poster, originally created as part of the assessment for the epidemiology module at LSHTM. I express sincere gratitude to Professor Neil Pearce for his exceptional teaching.\n","date":"2023-12-15T22:11:12+08:00","permalink":"https://haokaitseng.github.io/post/20231216-hpv-single-dose-vaccination/","section":"post","tags":null,"title":"Assessing the Non-inferiority of the Single-dose Human Papillomavirus Vaccination Schedule: A Hypothetical Multi-country Cohort Study"},{"categories":["Data Science"],"contents":" This article delivers two key insights:\nAdvantages of applying UK GDPR in research planning. Five approaches to safeguard data protection in lung cancer patient interviews. Setting th Scene The need of assessing the Quality of Life (QoL) in patients with lung cancer undergoing chemotherapy has been increasing. After treatment, patients may experience breathlessness or fatigue, along with potential challenges in their daily and occupational functioning. These side effects of chemotherapy consistently rank as a common complaint among patients. By using the validated QoL questionnaire from the European Organisation for Research and Treatment of Cancer, a hypothetical research project aim to summarize the QoL state among a cohort of patients in a hospital in the UK. Hopefully it can benefit the treatment of choice.\nHowever, concerns regarding data protection were brought up in a management meeting, with the focus on data confidentiality.\nMany patients consenting to participate in the study have uncertainty about their personal data processing, particularly in relation to sensitive questions. How do you lawfully and comprehensively tackle the data protection issue in a patient survey?\nData Governance in GDPR To manage such risks, the GDPR can be one of the best solutions that is inetegrated into the research planning and implementations. Implementing this information governance rules can yield numerous benefits, encompassing aspects such as safeguarding individual rights and freedom, fulfilling lawful obligations, and gaining hospital\u0026rsquo;s competitive advantages.\nConversely, non-compliance with GDPR may cause research failure, distress to individuals, fines and penalties, and damage to hospital\u0026rsquo;s reputation.\nGiven GDPR’s seven key principles below, I propose five approaches that center on the principle of integrity and confidentiality to mitigate information governance risks and remove doubts regarding data security. Procedures and policy for data processing security With regard to security in data processing, transfer, and storage, it can be achieved through technical procedures and organisational policy, such as encryption, pseudonymization, access control, and audit.\nInformation technology (IT) security remediation Through employing tools for reviewing IT security, system vulnerabilities can be identified and mitigated, along with fortified protection against potential security threats and cyber security events.\nProtocols for notifying a personal data breach Protocols need to be developed to respond to a personal data breach event, ensuring that the investigation and reporting are completed within 72 hours.\nAppointing a data protection officer This role’s responsibility is to oversee, monitor, and document operations related to data processing, applying extensive knowledge of data protection practices.\nA centralised area designated for data storage and processing To minimize the risk of data leak, data processing is exclusively and physically performed on these dedicated desktops in an office.\nFor more detailed you can refer to the GDPR’s Data Protection and Impact Assessment Table in the end of this post\nRecommended Approaches To balance between risks presented by processing personal data and research utility, I propose endorsement of the first three approaches as a selection of strategies for this study due to following reasons. First, they follow GDPR’s principle of integrity and confidentiality, a priority particularly valued by your patients.\nSecond, the advantages inherent in these approaches are substantial. For instance, the first and second approach can minimize the likelihood of negative events, such as unauthorized access or data breach; the third approach then serves as a response to such negative events.\nThird, the consequences of not implementing these approaches would be intolerable when weighed against their disadvantages, since a lack of robust safeguards for data protection can result in confidentiality risks, violation to regulations, and erode trust among patients.\nLastly, the estimated residual level of information governance risks (impact and probability) for the first three approaches are lower compare to the remaining approaches. A data protection officer (fourth approach) is considered more suitable for a large-scale study; a centralized area for data processing (fifth approach) runs a risk of accidental data corruption.\nConclusion Research planning can incorporate GDPR to help make trade-offs in data/information governance risks. While each GDPR’s principle is equally important, I recommend three approaches that can strengthen the compliance to the principle of integrity and confidentiality. By establishing procedures and policy, implementing IT security remediation, and generating protocols for notification, this hypothetical study can have appropriate safeguards in place to process data safely.\nFurthermore, I believe the assessment of QoL can provide the opportunity to improve QoL and symptom burden management of lung cancer patients with chemotherapy drugs. Therefore, we can not only accelerate research progress but also address confidentiality and regulatory requirements in our scope of research.\nData Protection and Impact Assessment Table What approach could be taken to reduce risk? What actions would this involve? What are the advantages and disadvantages of these actions for research? Approach 1: Procedures and policy for data processing security (1) Data processing involves the practice of protecting information from unauthorized access and corruption during the entire research lifecycle. (2)It encompasses robust encryption and pseudonymization of personal data, while in parallel ensuring confidentiality, integrity, and resilience of processing systems. (3)Regular audit and monitoring are required to promptly detect and address any anomalies or potential data breaches. (4)Encrypted data transfer is implemented, and data storage and network security are ensured through essential measures on physical hardware. (1) Advantages: (1-1)Implementing security procedures and policies helps mitigate the risks of accidental or unlawful destruction, alteration, loss, and unauthorized access. (1-2)It not only complies with Article 32 of the GDPR but also adheres to the accountability principle of the GDPR, as the security logs and documentation can be used to demonstrate compliance. (2) Disadvantages: Enforcing access controls could introduce complexity and potentially decrease efficiency within the research project. Approach 2: IT security remediation (1)IT security remediation is an actionable set by detecting and protecting against potential security vulnerabilities or cyber security events, such as unauthorized access or disclosure due to non-encrypted transmission of data. (2)It identifies existing and scheduled mitigations for security issue, and rank likelihood of the issue occurring given existing or scheduled mitigations. (3)It updates existing security and antivirus software and strengthens IT infrastructure to ensure mitigations and controls are in place. (1) Advantages: (1-1)Cybersecurity remediation can help manage risks and intercept IT security threats. This approach aligns with the National Cyber Security Centre (NCSC) Cyber Assessment Framework (CAF) guidance. (1-2)By identifying security concerns, it indirectly adheres to the integrity and confidentiality principle of the GDPR. (2)Disadvantages: Procurement and management of security software entails ongoing expenses. Approach 3: Protocols for notifying a personal data breach (1)In response to personal data breach, protocols and internal procedures should be well-planned, including pulling the facts, contain the breach, access the risk, and report it without delay, and act to protect affected patients. (2)After having become aware of personal data breach, the research team should notify the issue to the Information Commissioner, no later than 72 hours. (3)LSHTM’s Research Ethics Committees should also be notified when any breach of the study protocol or the principles of Good Clinical Practice has been identified. (1) Advantages: (1-1)According to Article 33 of the GDPR, reporting a personal data breach in a timely manner (\u0026lt;72 hours) can avoid fine (up to £8.7 million or 2 per cent of turnover) and penalties. (1-2)It meets the principle of lawfulness, fairness and transparency, accountability, and integrity and confidentiality in the GDPR. (1-3)A comprehensive procedure can save time to respond to an event of personal data breach and communication strategy (Article 34 of GDPR). (2)Disadvantage: Documented protocols and procedures are needed, along with essential training sessions for the research team. Approach 4: Appointing a data protection officer (1)The data protection officer is designated on the basis of data protection laws (GDPR) and practices for confidentiality. (2)The data protection officer is involved in all issues which relate to the protection of personal data, including regular and systematic monitoring of data subjects. (3)The data protection officer manages and maintains records of data analysis logs, as well as input and output activities within the designated area. (1)Advantages: (1-1)Compliance with regulations is regularly and systematically monitored, which aligns with the Article 37 of the GDPR and the principle of integrity and confidentiality and accountability of GDPR. (1-2)There is a contact point on issues relating to processing and protecting personal data. (2)Disadvantages: (2-1)The scope of this research is insufficient to warrant the role of a data protection officer, since systematic monitoring of data subjects is not conducted on a large scale. (2-2) Research budgetary allocation will be required to hire a data protection officer. Approach 5: A centralised area designated for data storage and processing (1)A structured set of personal data, including electronic health records, is stored in the filing system of the desktops in the information management department at the hospital. (2)Only members from the research have access to the research eligible patients’ personal data. (3)Data processing and analysis are exclusively and physically performed on these dedicated desktops. (1)Advantages: (1-1)It ensures that data remains confined to these systems and cannot be exported, minimizing the risk of data leak. (1-2)It provides protection against unauthorised processing of personal data, which aligns with the principle of integrity and confidentiality from the Article 5 of the GDPR. (2)Disadvantages: (2-1)There is a risk of accidental loss or corruption in the event of destruction or damage to the area of centralized data. (2-2)Remote work is not available. In collaboration with ChatGPT, this article has been adapted from the assessment of the Thinking like a Health Data Scientist module at LSHTM.\n","date":"2023-10-30T22:11:12+08:00","permalink":"https://haokaitseng.github.io/post/20231030-data-governance/","section":"post","tags":null,"title":"Optimizing Data Protection: Leveraging Information Governance Principles from GDPR into Research Planning"},{"categories":["Data Science"],"contents":"This year, London home rental prices hit record as demand outstriped supply, with the average tenant now being asked to pay a whopping £2,500 a month for a new let.\nWithin this rental market, individuals seeking short-term accommodations encounter greater challenges. My roommates and I was looking for a house/flat for co-renting with a 6-month tenancy, and we put in a significant effort throughout the entire month of August by searching and making inquiries about over 170 properties before finally settling down. Using Python in Visual Studio Code, I hereby analyze the data in our list of properties, which might provide an insight for your search and budget.\nWhat you will learn in this article:\nTrade-off between comunicating time and rent. Key factors that determine rent. Strategies to find your property. Photo by João Barbosa on Unsplash\nTrade-off Between Comunicating Time and Rent We browsed websites such as OpenRent, RightMove, Spotahome, and Zoopla during our search. We documented the features of 174 properties, capturing details including price per calendar month, tenancy, number of bedrooms, number of bathrooms, maximum number of tenants, location (zone), and comunicating time to my School (London School of Hygiene and Tropical Medicine) measured using Google Maps.\nFirst, the majority of rental properties in the market typically require a minimum one-year tenancy commitment, whereas our preference is for a shorter-term tenancy of more or less 6 months, with the option to extend if everything proceeds smoothly. Hence, among the targeted properties that specified their tenancy requirements, 81% state their acceptance of a minimum 6-month tenancy.\nHere I provide a overview of our data by price, communicating time, and the number of bedrooms. The range of rent is between £1,850 to £4,500, while the range of communicating time is from 17 to 67 minutes.\nIf you are also sensitive to both cost and time, you can observe a noticeable pattern through simple linear regression. It suggests that you need to invest approximately £19 in order to gain an additional minute of time saved on your commute to school.\nTo validate the assumption of normality and homoscedasticity in the regression model, I make QQ-plot and random clouds. They seem pretty good.\nI also use K-means clustering algorithm to find the most suitable number of clusters according to rental costs. There is a substantial disparity between the outcome obtained using the elbow method and the Silhouette Score. The elbow method indicates that 4 clusters would be optimal, aligning somewhat with the established property zones. In contrast, the Silhouette Score suggests for a more fine-grained segmentation into 9 groups.\nKey Factors That Determine Rent Aside from communicating time, many factors also contribute to our living qualities. When conducting a multiple linear regression analysis, I identify that several variables significantly impact rent:\nProperty Zone: Choosing to reside in Zone 2 instead of Zone 1 can result in a savings of £319, while opting for Zone 3 over Zone 2 can lead to a savings of £102. Number of bedrooms: Acquiring an additional bedroom is associated with an increased cost of £384. Number of bathrooms: An additional bathroom comes with an added expense of £62. On the other hand, certain factors appear to have less influence on rent, including directional location of the property, maximum number of tenants, and minimum tenancy.\nStrategies to Find Your Property When seeking short-term accommodation in London it\u0026rsquo;s imperative to dedicate substantial effort to locate and secure your perfect property within this highly competitive rental market. In the following sections, I will outline three useful strategies that can enhance your search and help you save valuable time:\n1. Establish Your Financial Parameters and Objectives Begin by determining your budget constraints and the anticipated duration of your stay. Identify essential amenities that align with your preferences, such as a two-bathroom layout, laundry facilities, access to a rear garden, or parking availability. Additionally, be sure to specify your desired move-in date.\nIf your plan involves a shorter tenancy, be prepared for the possibility of residing further away from the center of London. Generally, properties locate in Zone 3 or the outskirts of London are more amenable to short-term tenancies. In addition, many landlords may require you to make an upfront payment of the total rent, if you lack a job, a reliable financial history, or references.\n2. Utilize Appropriate Online Platforms Consider exploring channels such as Spotahome, OpenRent, and various Facebook groups for your search. Spotahome operates in a manner similar to Airbnb, offering a comprehensive online process prior to your arrival. OpenRent may have a little landlords open to short-term tenancies, especially those looking to rent out properties located in remote or less competitive areas.\nHowever, exercise caution in Facebook rental groups, as scams can be a concern.\nMainstream websites like RightMove often feature properties gear toward long-term rentals, so they may not align with your short-term needs.\nLastly, don\u0026rsquo;t hesitate to contact a local agency to seek their assistance in locating a suitable property. Nestify is a good choice, since all of their properties are available for 3-9 months.\n3. Act Swiftly In the fast-paced environment, it\u0026rsquo;s essential to act swiftly to secure your desired house. You should regularly check rental websites that I mentioned for new listings everyday. Hot properties get inundated with inquiries. Send a detailed introduction to the landlord or agency.\nOnce a viewing is confirmed, whether in-person or virtually, make your decision promptly, I mean, in hours! Stay proactive throughout the process of paying deposit and signing contract. Delaying the process could result in the property being offered to other interested parties.\nConclusion I exemplify the know-how for searching for short-term accommodation London. Drawing on evidence-based analytics, this article offers data-driven insights into several factors influencing the cost of short-term rentals.\nNavigating the complex world of real estate in London can be daunting, but this article aims to anchor readers in their decision-making process.\nWishing you the very best of luck in your search!\nThis article is also published on Medium。\n","date":"2023-09-09T22:11:12+08:00","permalink":"https://haokaitseng.github.io/post/20230907london-rent-cost/","section":"post","tags":null,"title":"Navigating Short-Term Accommodation Search in London"},{"categories":["Essay"],"contents":"你知道，台灣是全球受境外假訊息侵擾最嚴重的國家，而且已經蟬聯榜首10年嗎？\n本文選台灣的生化實驗室為例，揭露一個境外資訊操弄與干預的手法。\n謠言的前世 從2020年COVID-19疫情以來，在當時的Twitter(現稱X)渠道上，就流傳一個謠言是，美國在全世界設的生化實驗室被陸續曝光，包含台灣的10座實驗室詳細名稱、地址、聯絡資訊等。\n但這訊息已有Cofacts真的假的和MyGoPen闢謠，指出「美國在台設實驗室做生化武器或研究大規模流行病」屬於陰謀論，企圖煽動與分化。\n台灣這則謠言的前身，是來自俄烏戰爭的假訊息。\n俄國曾指控美國在烏克蘭開設生化實驗室，俄烏戰爭開打後，還緊急銷毀致命的病原體。但實情是，美國在戰前就推動在烏克蘭的公衛及生物威脅減少計畫，並非研發生物武器，且銷毀只是避免實驗樣本落入俄軍手中。\n可見這次的陰謀論版圖擴大，把俄烏戰爭的謠言巨網張到台灣的上空。\n謠言的今生 今年7月，聯合報獨家報導美方要台灣開設P4實驗室研發生物戰劑。前立委雷倩亦在中天新聞的節目上表示，因為台灣人可以代表全中國人的DNA，所以美方在此研發生物戰劑是危險的。\n縱使國防部、國安局、外交部、美國在台協會立刻聲明此傳聞非事實，台灣曾簽署「禁止發展、製造、儲存、取得或保有生物及毒素武器並避免其可能引發之毀滅公約」，故不研發生物戰劑，且台北地檢署後續於9月5日認定所謂「南海工作會議紀錄」實屬造假，包括使用了許多中國式用語等。但雷倩的疑美片段被轉傳再製，最再加上微博大V玉渊谭天(實為中國央廣旗下的自媒體品牌)的推波助瀾，讓聲量達微博的135萬影片觀看人次：\n「真正被犧牲的到底是誰？」 ── 玉渊谭天《独家披露台湾生物实验室分布》(2022/8/16)\n玉渊谭天不但暗指台灣人民就是美國與台灣政府勾連下最大的犧牲者(因為台灣P2等級以上實驗室多在人口稠密區儲藏病毒)，還對中國人民恫嚇台灣病毒的威脅(台灣實驗室多位於靠近中國的西半部)，還附上繪聲繪影的證據(國防部預醫所增加的預算是為了建P4實驗室，以及預醫所疑似動土前後的空照圖)。陰謀論、疑美論、與誇張描述的手法可見一斑。\n在Facebook的繁體字社群，也發現一些異常操作。周曉彤轉貼來自Tiktok的短影片，爆料美國要在台灣建造P4實驗室，但其帳號本身可疑，因其大頭貼顯為盜用；其發文還被李倩(帳號同樣不真)分享到15個反綠陣營社團，內容完全複製相同，顯非一般自然人的行為模式(Pattern of Behavior)。 根據上述資料，可以看出境外資訊操弄與干預(FIMI, Foreign Information Manipulation Interference)的痕跡，其中串聯了議題、行動者、動機(利益)、以及行動四大要素。雖然微博大V還透過標籤劫持(Hashtag Hijacking)來吸引注意，但這事件在台灣主流社群平台上聲量相對不高(至多在中天新聞上傳於Youtube的節目片段有5萬人次觀看)，討論度熱度也不持久。\n有趣的是，今年8月美國疾病管制單位在加州破獲一間非法生物實驗室，其中有實驗老鼠、化學藥品及帶有傳染源的生物材料。這起非法實驗室疑與中國有關，引爆美國的國安危機，同時亦可做為「台灣生化實驗室」謠言的真實對造組。\n結語 透過SCOTCH框架調查發現，兩岸媒體以及非國家行動者使用Youtube、Facebook、X(Twitter)、微博、Tiktok等社群平台，引用台媒的獨家報導及台灣的實驗室相關圖資，通過標籤劫持來吸引注意，以及在有共同理念(Shared-Interest)的Facebook社團中複製貼上分享貼文。 試圖在中國群眾及台灣反對民進黨的群眾中，植入美方籲台建造P4實驗室研發生物戰劑的謠言，這謠言帶有部分真實性，但滲入不實資訊或過度解讀，以植入民進黨政府犧牲民眾安全的疑美論，不但破壞台灣民眾對國家生物安全政策的信任，也令中國群眾感受到來自台灣的生化威脅。\n本文使用的工具及分析方法感謝台灣民主實驗室的課程。\n本文同步發表於Medium。\n","date":"2023-08-31T22:11:12+08:00","permalink":"https://haokaitseng.github.io/post/20230901%E5%8F%B0%E7%81%A3%E7%94%9F%E5%8C%96%E5%AF%A6%E9%A9%97%E5%AE%A4/","section":"post","tags":null,"title":"有關台灣生化實驗室的境外資訊操弄與干預 / A Case Study of Foreign Information Manipulation Interference: United States Asked Taiwan to Develop Weaponized Biological Agents in P4 Lab"},{"categories":["Essay"],"contents":"1. 全球總人口當中，有3.5%的人是移工。但在COVID-19(下稱新冠肺炎)確診數最高的20國當中，移工佔其總人口數至少4.5%，這些移工在疫情當中相對地面臨更高的感染風險以及生存考驗。\n早於疫情之前，移工容易成為社會中的弱勢族群。但疫情使得這群人更為脆弱，原因包含邊境的封鎖、產業緊縮導致的失業或勞動條件劣化、集體式(易群聚感染)的住所、對健康資訊的語言識讀障礙、身分變動造成在社會安全網邊緣化或排除、還有對外來者的歧視等等。\n我們可以從跨境匯款的消長來想像移工經濟影響的程度。根據世界銀行在去年四月的推估，2020年匯往中低收入國家的款項估計較原先預估金額大減20%，雖然在後來10月的報告中有對此比例下調為11%，但這幅度還是相當驚人，代表許多移工出外打拼，卻可能因為疫情失業，賺不夠錢寄回老家供養。但有些不尋常的金流，例如尼泊爾在下半年度收到較往年更多的匯款，推測與當地疫情更趨嚴重，海外家人加強金援有關。\n受影響的還有國際照顧鍊的末端──已開發國家的勞動力需求。日本目前因全面鎖國，營造業與服務業的人力短缺立刻浮現；韓國的農工等也面臨短缺，尤其像季節性移工完全無法入境，反倒讓現有的農工日薪漲了兩三成。\n於是報導常用stranded一詞，來形容在疫情中大規模擱淺的移工，不論是在人身自由、收入、或思鄉的心情上。\n2. 但在臺灣──一個好一段時間被新冠病毒遺忘的島嶼，這詞蘊含的景況全然不同。\n指揮中心表示，案〇〇〇為印尼籍20多歲男性，今(2020)年〇〇月〇〇日來臺工作(〇〇〇〇)，持有登機前3日內核酸檢驗陰性報告，入境後至防疫旅館進行居家檢疫，迄今無症狀；〇〇月〇日檢疫期滿後，搭乘專車至其他處所自主健康管理，〇〇月〇〇日由公司安排自費採檢，於今日確診(Ct值〇〇)。衛生單位已匡列個案接觸者共〇人，因有適當防護，列為自主健康管理。\n這是我一個疫調到半夜的個案，最後在新聞稿上呈現的段落。不為人知的是，那些列為自主健康管理的接觸者，與個案同一時期入境，一起搭車採檢，跟個案也語言不通，很快就被外商公司遣送回國了。當時我連忙跟仲介解釋，他們有著入境陰性+14天檢疫+7天自主自康管理+口罩+自費陰性的金牌，我們防疫並沒有需要做到這個地步，有其他例如再次自費的辦法來解決…。「公司很謹慎」，仲介只能無奈地回應。\n相較於接觸者是不被停留的人，這位印尼男孩則是徹底地被滯留了。受過大學教育的他，來臺灣工作是他出社會後第一份合約，檢疫/管理約20天後，再接一個月的住院隔離，中途歷經一度陰性卻陽轉的絕望。最終指揮官核定解隔後，他表示想回家(公司也不願用他)，但過了數週仍登不上飛機，因為搭機需要的核酸陰性報告，他出院驗了兩次還是生不出來(輕症者可能痊癒後數個月仍驗得出病毒核酸)。\n他因為邊境管制政策沒有隨新知調整，就這樣被持續被軟禁在旅館，此刻還在等待我們與印尼官方的協商結果。另一個協調成功的案例是，一位確診移工二採陰出院後，需要做外籍移工入境健檢，卻被多家醫院拒絕，最後有賴我老闆出面才有醫院願意幫她做健檢。\n對於仲介而言，移工縱使有搭機前陰性報告，還是與危險劃上等號，檢疫完還得公費/自費採檢才能放心；再次採檢的移工就是金雞母，仲介會追問我們報告結果，才能趕快交工抽成；但一旦陽性，移工就成了掃把星，仲介會被追殺做疫調(因為語言不通需仰賴翻譯老師)，好一點仲介的會在寒流時關心同鄉移工穿得夠不夠暖，差一點的仲介就不聞不問，或拒絕移工在住院期間想要的額外飲食花費，甚至有移工被仲介罵哭，說「妳為什麼要帶病毒來臺灣」。\n「(確診)這樣誰敢用？」這常是仲介接到噩耗當下的第一反應。\n從1992年頒布\u0026lt;就業服務法\u0026gt;，臺灣引進產業與社福類移工快滿30年，但因為活動領域顯有交集，70萬移工對許多本地人而言是隱形或(因為聽不懂)刺耳的存在。\n是疫情讓潛在30年的歧視浮現與變質。從去年年底因印尼移工佔境外移入的大宗，指揮中心宣布全面暫停引進印尼籍移工，疫情新聞下方就總會有要移工滾回母國、別帶病毒來台、健保不該補助醫療費用等仇外留言。我們也常接到民眾陳情哪裡有居檢的移工亂跑，或某公眾場所有移工群聚，或說移工對社區的老人小孩很危險云云。\n另一方面，疫情也讓移工的脆弱性有所反轉。因為許多航班停飛、減班、或漲價，聘僱期滿逾期居留的外籍移工無法遣送離境，導致收容中心被擠爆，再加上收容替代措施，非法移工便可以暫時合法地打黑工，黑市薪資水漲船高。而國際照顧鍊的斷層，讓有長照需求的家庭有錢也找無工，引發其他社會問題。\n3. 在傳統傳染病領域，藍領移工本就是政府嚴加監控的對象。像我們部門需花多達三分之一的人力，以高額罰鍰輔導仲介記得多次的移工定期健檢，來篩檢出寄生蟲、結核病等境外移入傳染病；現因應新冠肺炎疫情，疾管署有製作多國語言素材的防疫宣導品，還與勞動部啟動泰、越籍移工檢疫期滿篩檢等加強措施。\n但這樣就足夠了嗎？私認為一旦桃園守不住，社區開始流行，兩類移工可能成為傳播加速器：一是住團體宿舍的產業類移工，二是無合法身分的移工。\n新加坡就是第一類移工成為疫情破口的借鑑。去年4月爆發的大規模移工宿舍群聚感染，就是因為宿舍環境長期壅擠不人性化所導致；他們最終在去年12月以核酸與血清檢測大量篩檢發現，移工族群的新冠肺炎盛行率高達47%(相當於1萬5千人確診)，對比一般市民僅4千多的確診數，可為懸殊。但危機也可以是轉機，新加坡也因為疫情開始重視外籍移工的居住空間，調整使宿舍房間大小與密度符合國際勞動組織的規範。\n第二類移工曾在去年疫病濫觴時成為焦點。一位非法的移工看護在醫院染疫，引發相關部門加強查緝的動作，但最後指揮官因防疫量能的考量，反對掃蕩非法看護而作罷。\n上周才剛接到一通雇主說剛檢疫完的移工逃跑的電話，問這樣會不會造成防疫漏洞？我只能說我不知道。但我們更應該關心的是移工為什麼會逃跑的結構問題，這顯現我們修訂諸多勞動政策──取消移工三年出國一日、零付費改革、直聘等，仍無法根除仲介體制的剝削。唯有透過讓非法移工(無論是逃跑或聘期滿回不去)取得合法身分，才能促使移工重新被管理，願意通報與檢疫。\n你願不願意告訴我你生體不舒服，且不顧慮坦白之後的後果呢？\n一個防疫體制的成功與否，就在於它對其所有成員的可信賴性。病毒宿主並不分種族膚色，因此這信賴網絡文化的維護，不僅是勞動單位、仲介、雇主他們的事，這也是我們的事。\n本文亦同步發表於Medium\n","date":"2021-02-01T22:11:12+08:00","permalink":"https://haokaitseng.github.io/post/20210201%E7%96%AB%E6%83%85%E4%B8%8B%E7%9A%84%E7%A7%BB%E5%B7%A5-%E6%B5%81%E9%9B%A2%E5%B0%8B%E5%B2%B8%E7%9A%84%E4%BA%BA/","section":"post","tags":null,"title":"疫情下的移工，流離尋岸的人 / Migrant Workers Are Displaced Under COVID-19 Pandemic"}]